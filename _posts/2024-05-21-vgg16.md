---
title: Very Deep Convolutional Networks for Large-Scale Image Recognition
---
# 1. INTRODUCTION

이 논문은 깊이가 깊어질수록 이미지 인식의 정확도가 향상된다는 것을 보여주며, 간단한 ConvNet 구조를 사용해도 깊이를 늘리면 ImageNet 챌린지 데이터 세트에서 최첨단 성능을 달성할 수 있음을 입증했습니다. 또한, 이 모델이 다른 작업 및 데이터 세트에도 잘 일반화되어 복잡한 이미지 표현 파이프라인보다 성능이 뛰어나다는 것을 보여주면서 시각 표현에서 깊이의 중요성을 다시 한 번 확인했습니다.

레이어의 깊이, 모든 레이어에 3x3 conv 필터를 동일하게 적용

3x3 필터를 고정적으로 사용하는 이유
1. 3x3 필터로 2번 convolution하는 것과 5x5 필터로 1번 convolution하는 것은 동일한 사이즈의 feature map을 나타냄
2. 채널 수를 C라고 가정했을때 VGGnet에서 3x3 필터를 사용한 이유는 3x3필터 3개를 쌓았을경우 총 27×C^2개의 가중치 파라미터를 갖고, 
7×7 필터를 사용할 경우 49×C^2의 가중치 파라미터를 갖는다. 즉 연산량을 줄일 수 있다.
3. 1개의 큰 필터 레이어를 거치는 것보다 3개의 작은 필터를 거치면서 비선형성을 증가시킬 수 있다.


# 2. CONVNET CONFIGURATIONS

## 2.1. ARCHITECTURE
* 입력 : 224x224 RGB 이미지
    * 입력 이미지에 대한 전처리 과정은 RGB mean Value를 빼주는 것만을 적용
    ※ RGB mean Value : 이미지 상의 pixel들이 가지고 있는 R,G,B 각각의 평균값
* convolution layer 
    * 3x3 필터 주로 사용 : 3×3이 left,right,up,down 4가지 방향을 고려할 수 잇는 최소한의 receptive field이기 때문에
    * 1x1 필터 사용 : 비선형성 증가를 위해
* pooling layer : 공간 해상도 감소
* fully connected layer : 이미지 분류
* 활성화 함수: ReLU

## 2.2. CONFIGURATIONS
![table1]()

conv3 : 3x3 필터
conv1 : 1x1 필터
conv3-N : N개의 3x3 필터

각 열은 하나의 구성을 나타내며, 깊이는 왼쪽(A)에서 오른쪽(E)으로 갈수록 깊어집니다. 굵게 표시된 레이어는 이전 구성에서 추가된 레이어를 나타낸다.

신경망의 마지막 3 Fully Connected Layer(FC)는 각각 4096, 4096, 1000개의 유닛으로 구성되어 있으며, 출력 층(1000)은 classification을 위한 softmax함수를 사용한다.

![table2]()

각 ConvNet 구성의 매개변수 수를 백만 단위로 보여준다. 네트워크 A와 A-LRN은 1억 3,300만 개의 매개변수를 가지고 있으며, 가장 깊은 네트워크 E는 1억 4,400만 개의 매개변수를 가지고 있다. 깊이가 깊어짐에도 불구하고 매개변수의 수는 크게 증가하지 않는다. 이는 conv의 너비가 좁기 때문이다.

## 2.3. DISCUSSION
본 논문에서는 기존 연구와는 다르게 첫 번째 컨볼루션 레이어에 비교적 큰 수용 필드 대신 전체 네트워크에 걸쳐 매우 작은 3x3 수용 필드를 사용하여 모든 픽셀에서 입력을 처리합니다. 이는 3x3 컨볼루션 레이어를 여러 개 쌓는 것이 단일 7x7 컨볼루션 레이어보다 더 많은 장점을 제공하기 때문입니다. 
첫째, 여러 개의 비선형 정류 레이어를 통합하여 결정 함수를 더욱 구별할 수 있게 만들고, 
둘째, 3x3 필터를 통해 7x7 필터를 분해하도록 강제하여 매개변수 수를 줄입니다. 
또한, 1x1 컨볼루션 레이어를 사용하여 수용 필드에 영향을 주지 않으면서 결정 함수의 비선형성을 높입니다. 
이러한 작은 필터 크기의 컨볼루션은 이전 연구에서도 사용되었지만, 본 논문에서는 훨씬 더 깊은 네트워크를 사용하고 대규모 ILSVRC 데이터 세트에서 평가했다는 점에서 차별성을 가집니다.


# 3. CLASSIFICATION FRAMEWORK 
## 3.1 TRAINING
* Optimization: multinomial logistic regression using mini-batch gradient descent with momentum (batch size = 256, momentum to 0.9)
* Regularization : weight decay(L2 Norm with 5*10^-4), dropout = 0.5 (for first 2 FC-Layer)
* Learning rate : 0.01 (단,validation set에 대한 accuracy가 증가하지 않을 시, 10배로 감소
해당 실험에서는 3번 감소하였으며, 74 epoch 학습)

ConvNet 훈련은 역전파와 모멘텀을 사용하는 미니 배치 경사 하강법을 통해 다항 로지스틱 회귀 목표를 최적화하여 수행됩니다. 훈련 과정은 가중치 감소 및 드롭아웃 정규화를 통해 정규화되며, 검증 세트 정확도가 향상되지 않을 때 학습률이 감소합니다. 
깊이가 깊어짐에도 불구하고, 더 큰 깊이와 더 작은 컨볼루션 필터 크기로 인한 암묵적인 정규화와 특정 레이어의 사전 초기화를 통해 네트워크는 더 적은 epoch으로 수렴할 수 있습니다. 

### Training image size
훈련 이미지는 무작위로 잘리고, 뒤집고, 색상이 변경되어 훈련 세트를 더욱 확장합니다. 훈련 이미지 크기는 단일 척도 또는 다중 척도로 설정할 수 있으며, 다중 척도 훈련은 다양한 척도의 객체를 인식하도록 모델을 훈련하는 데 유용합니다.

## 3.2. TESTING
이미지 크기 조정
이미지의 가장 짧은 변의 길이 : Q (테스트 척도)
fully connected layer -> convolution layer 변환하여 전체 이미지에 적용. 이를 통해 이미지 전체























---