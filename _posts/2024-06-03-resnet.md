---
title: 논문리뷰_ResNet
categories: [논문리뷰] 
date: 2024-06-03
last_modified_at: 2024-06-05
---

논문출처 : [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385)


# 1. Introduction
잔차 학습(Residual Learning)을 통한 심층 신경망 성능 향상

* 문제점:
    * Vanishing Gradient (Exploding Gradient, 기울기 소실/폭발): 심층 신경망은 깊이가 깊어질수록 기울기 소실/폭발 문제로 인해 학습이 어려워짐.
    * 성능 저하: 깊이 증가에 따라 훈련 오류가 증가하여 정확도가 떨어짐 (overfitting(과적합) 때문이 아님).
* 해결책: Residual Learning(잔차 학습):
    * 핵심 아이디어: 여러 레이어를 쌓아 원하는 함수를 직접 근사하는 대신, 입력값 x에 대한 잔차 함수 F(x) := H(x) - x를 학습. 즉, 원래 함수 H(x)를 F(x) + x로 표현.
    * 장점: 잔차 매핑을 최적화하는 것이 더 쉬움. 특히 항등 매핑이 최적일 경우 잔차를 0으로 만드는 것이 더 용이.
    * 구현: shortcut connections를 사용하는 feedforward neural networks로 구현 가능 (추가 매개변수나 계산 복잡성 없음).
* 실험 결과:
    * 최적화 용이성: 매우 깊은 residual net은 최적화하기 쉬우나, 단순히 레이어를 쌓는 "일반(plain)" net은 깊이 증가 시 훈련 오류 증가.
    * 정확도 향상: 깊은 residual net은 깊이를 크게 늘려 정확도를 쉽게 높일 수 있으며, 기존 네트워크보다 훨씬 더 나은 결과 생성.
* 결론:
    * ImageNet 데이터 세트에서 residual net을 사용하여 뛰어난 성능 달성.
    * 잔차 학습 원리는 다양한 문제에 적용 가능함을 시사.
* 핵심: 잔차 학습은 심층 신경망의 성능 저하 문제를 해결하고 매우 깊은 네트워크 학습을 가능하게 하여 이미지 인식 등 다양한 분야에서 뛰어난 성능을 보임.

![figure1]()
![figure2]()


# 2. Related Work
## 1) Residual Representations (잔차 표현)
이미지 인식에서 VLAD [18]는 사전과 관련하여 잔차 벡터로 인코딩하는 표현이며, Fisher Vector [30]는 VLAD의 확률적 버전 [18]으로 공식화될 수 있습니다. 둘 다 이미지 검색 및 분류를 위한 강력한 얕은 표현입니다 [4, 48]. 벡터 양자화의 경우 잔차 벡터 [17]를 인코딩하는 것이 원래 벡터를 인코딩하는 것보다 더 효과적인 것으로 나타났습니다. 저수준 비전 및 컴퓨터 그래픽에서 편미분 방정식(PDE)을 풀기 위해 널리 사용되는 다중 격자 방법 [3]은 시스템을 다중 스케일의 하위 문제로 재구성하며, 각 하위 문제는 더 거친 스케일과 더 미세한 스케일 간의 잔차 솔루션을 담당합니다. 다중 격자의 대안은 계층적 기반 사전 조정 [45, 46]으로, 두 스케일 간의 잔차 벡터를 나타내는 변수에 의존합니다. 이러한 솔버는 솔루션의 잔차 특성을 인식하지 못하는 표준 솔버보다 훨씬 빠르게 수렴하는 것으로 나타났습니다 [3, 45, 46]. 이러한 방법은 좋은 재구성 또는 사전 조정이 최적화를 단순화할 수 있음을 시사합니다.

## 2)





























---