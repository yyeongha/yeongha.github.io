---
title: 논문리뷰_ResNet
categories: [논문리뷰] 
date: 2024-06-03
last_modified_at: 2024-06-05
---

논문출처 : [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385)


# 1. Introduction
잔차 학습(Residual Learning)을 통한 심층 신경망 성능 향상

* 문제점:
    * Vanishing Gradient (Exploding Gradient, 기울기 소실/폭발): 심층 신경망은 깊이가 깊어질수록 기울기 소실/폭발 문제로 인해 학습이 어려워짐.
    * 성능 저하: 깊이 증가에 따라 훈련 오류가 증가하여 정확도가 떨어짐 (overfitting(과적합) 때문이 아님).
* 해결책: Residual Learning(잔차 학습):
    * 핵심 아이디어: 여러 레이어를 쌓아 원하는 함수를 직접 근사하는 대신, 입력값 x에 대한 잔차 함수 F(x) := H(x) - x를 학습. 즉, 원래 함수 H(x)를 F(x) + x로 표현.
    * 장점: 잔차 매핑을 최적화하는 것이 더 쉬움. 특히 항등 매핑이 최적일 경우 잔차를 0으로 만드는 것이 더 용이.
    * 구현: shortcut connections를 사용하는 feedforward neural networks로 구현 가능 (추가 매개변수나 계산 복잡성 없음).
* 실험 결과:
    * 최적화 용이성: 매우 깊은 residual net은 최적화하기 쉬우나, 단순히 레이어를 쌓는 "일반(plain)" net은 깊이 증가 시 훈련 오류 증가.
    * 정확도 향상: 깊은 residual net은 깊이를 크게 늘려 정확도를 쉽게 높일 수 있으며, 기존 네트워크보다 훨씬 더 나은 결과 생성.
* 결론:
    * ImageNet 데이터 세트에서 residual net을 사용하여 뛰어난 성능 달성.
    * 잔차 학습 원리는 다양한 문제에 적용 가능함을 시사.
* 핵심: 잔차 학습은 심층 신경망의 성능 저하 문제를 해결하고 매우 깊은 네트워크 학습을 가능하게 하여 이미지 인식 등 다양한 분야에서 뛰어난 성능을 보임.

![figure1]()
![figure2]()


# 2. Related Work
## 1) Residual Representations (잔차 표현)
이미지 인식 분야에서 VLAD와 Fisher Vector는 이미지를 숫자 벡터로 변환하는데, 이때 이미지의 특징을 잘 나타내는 잔차 벡터(residual vector)를 사용합니다. 잔차 벡터는 이미지의 특징을 더 효과적으로 표현하여 이미지 검색이나 분류에 유용하게 활용될 수 있습니다.

비슷한 개념이 컴퓨터 그래픽이나 영상 처리 분야에서도 사용됩니다. 예를 들어, 복잡한 이미지를 여러 단계로 나누어 처리하는 다중 격자 방법(Multigrid method)이 있습니다. 각 단계에서는 이전 단계에서 처리하지 못한 잔차(residual) 정보를 처리하는 방식으로 이미지를 점점 더 선명하게 만듭니다.

이처럼 잔차를 활용하는 방식은 이미지 처리 속도를 높이고 결과를 향상시키는 데 도움이 됩니다. 잔차는 기존 정보와의 차이를 나타내므로, 이를 활용하면 이미지의 세부적인 특징을 더 효과적으로 표현하고 처리할 수 있습니다.

## 2) Shortcut Connections
Shortcut Connections는 신경망에서 층들을 건너뛰는 연결을 말하며, 이는 오래전부터 연구되어 온 개념입니다. 초기에는 다층 퍼셉트론(MLP)을 훈련할 때 입력층에서 출력층으로 직접 연결되는 선형 레이어를 추가하는 방식으로 사용되었습니다. 또한, 몇몇 중간층들을 보조 분류기에 직접 연결하여 기울기 소실/폭발 문제를 해결하는 데도 활용되었습니다.

본 논문에서 다루는 ResNet과 유사한 시기에 개발된 Highway Networks는 게이트(gate)라는 기능을 가진 shortcut connection을 사용합니다. 게이트는 특정 조건에 따라 정보를 통과시키거나 차단하는 역할을 합니다. 하지만 ResNet은 게이트가 없는 항등 shortcut connection을 사용합니다. 즉, ResNet에서는 모든 정보가 항상 shortcut을 통해 전달되고, 추가적인 잔차 함수만 학습됩니다. 게이트가 없는 ResNet은 100개 이상의 층을 쌓아도 정확도가 향상되는 것을 보여주었지만, highway networks는 그렇지 못했습니다.


# 3. Deep Residual Learning
## 3.1. Residual Learning





























---