---
title: [kospeech] Open-source toolkit for end-to-end Korean speech recognition 논문 리뷰
---

논문 링크 : https://www.sciencedirect.com/science/article/pii/S2665963821000026

코드 링크 : https://github.com/sooftware/kospeech?tab=readme-ov-file

* end-to-end 
: 입력에서 출력까지 파이프라인 네트워크 없이 신경망으로 한번에 처리한다는 의미이다. 

![end-to-end](https://images.velog.io/images/jeewoo1025/post/c07c47d5-fc1b-4212-9a08-193646604898/image.png)


## 1. Introduction
 전통적인 자동 음성 인식(Automatic Speech Recognition, ASR) 시스템은 음향 모델, 어휘 사전 및 언어 모델을 포함합니다. 이러한 매우 복잡한 구조는 단일 디코딩 네트워크를 구축하는 데 사용됩니다.
 반대로 end-to-end ASR 시스템은 많은 장점을 가지고 있습니다. 우선 위에 작성된 모든 프로세스를 하나의 시스템으로 관리하기 때문에 훨씬 편리합니다. 그래서 deep speech 2, Listen, Attend and Spell (LAS), Transformer, RNN-Transducer 및 Joint CTC-Attenetion LAS 등 많은 end-to-end 모델이 제안되었습니다.
 도메인 지식 없이도 접근할 수 있을 정도로 간단하고 모델 구조가 명확하고 간결하기 때문에 더 직관적입니다. ASR의 다양한 구현이 그렇게 공개됩니다. 그러나 LibriSpeech, WSJ, Switchboard, CallHome과 같은 이러한 오픈 소스의 대부분은 영어와 같은 비한국어를 다루고 있습니다. 한국어 ASR 오픈 소스의 부재는 한국어 음성 인식의 진입 장벽을 높이는 주요 요인 중 하나가 되었습니다.
 따라서 지금까지 출시된 한국어 음성 데이터 세트 중 가장 큰 KsponSpeech를 처리할 수 있는 툴킷인 Kospeech를 열기로 결정했습니다. KsponSpeech는 1000시간 분량의 음성 데이터 전사 쌍으로 구성되어 있습니다. 

* ksponspeech 논문 : https://www.mdpi.com/2076-3417/10/19/6936

KsponSpeech의 텍스트 전처리가 어렵기 때문에 KsponSpeech 데이터 세트에 대한 전처리 방법도 설명합니다. 사용자는 음성 전사와 철자 전사 중에서 선택하는 방법이나 특수 문자를 제거하는 방법 등을 참조할 수 있습니다.
 또한 다양한 유닛에서 한국어 음성 인식을 수행하려는 연구자를 위해 문자, 하위 단어 및 자소 단위의 데이터 세트를 처리하는 방법을 구성했습니다. 모델 부분에서 KoSpeech는 딥 러닝 기반 end-to-end 음성 인식 모델인 Deep Speech 2, LAS, Transformer 및 Joint CTC-Attention LAS를 제공합니다.
 또한 특징 추출 방법, 순환 신경망(RNN), 장단기 메모리(LSTM), 게이트 순환 유닛(GRU)과 같은 순환 신경망 유형과 같은 다양한 옵션이 제공됩니다. 이들은 모델 성능과 높은 상관 관계가 있습니다.
연구자들은 다양한 옵션으로 음성 인식 작업을 실험할 수 있습니다. 또한 연구자의 시간과 비용을 절약하기 위해 사전 훈련된 모델과 사전 처리된 녹취록을 무료로 개방했습니다. 우리는 이 소프트웨어를 계속 개발할 것입니다. 우리의 연구가 팔로워와 초보자를 위한 지침이 될 수 있기를 바랍니다.


## 2. ASR system
 ASR 작업은 상당히 복잡한 파이프라인으로 구성되어 있습니다. KoSpeech는 특징 추출, 모델 훈련 및 디코딩과 같은 모든 복잡한 파이프라인을 지원하며 사용자에게 이 과정에서 필요한 옵션을 선택할 수 있도록 제공합니다. KoSpeech에는 50개 이상의 옵션이 있지만 세 가지 중요한 사항에 대해서는 간략하게 설명합니다. 위의 세 가지 옵션 외에도 옵티마이저, 모델 차원 및 손실 함수와 같은 옵션이 있습니다.

### 2.1. Output-unit
 음성 인식 작업에서 출력 단위는 중요한 하이퍼 파라미터입니다. KoSpeech는 표 1에 시각화된 문자, 하위 단어 및 자소 단위의 처리를 제공합니다. 연구자는 원하는 출력 단위를 지정할 수 있습니다.

![Table1](https://github.com/yyeongha/yyeongha.github.io/blob/main/assets/img/favicons/2024-04-30-kospeech/Table1.png?raw=true)

### 2.2. Model architecture
 KoSpeech의 모델 아키텍처는 다음과 같습니다. (a) Deep Speech 2, (b) Listen Attend and Spell, (c) Transformer, (d) Joint CTC-Attention LAS 

#### (a) Deep speech 2
 Deep Speech 2는  Connectionist Temporal Classification (CTC) 손실이 있는 ASR 작업에서 더 빠르고 정확한 성능을 보여주었습니다
(그림 1 참조).

![Fig. 1](https://github.com/yyeongha/yyeongha.github.io/blob/main/assets/img/favicons/2024-04-30-kospeech/Fig.%201.png?raw=true)

 이 모델은 이전 end-to-end 모델에 비해 훨씬 우수한 성능으로 강조되었습니다(그림 2 참조).


---