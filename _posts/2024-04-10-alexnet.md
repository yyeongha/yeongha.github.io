---
title: AlexNet 논문 리뷰
use_math: true
---
논문 출처 : https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html

ImageNet 데이터셋은 22,000개의 카테고리가 있는 150만개의 고해상도 이미지이다. 2010년부터 ILSVRC 대회에서는 ImageNet의 사진 중에서 약 1000개의 카테고리 별로 1000개의 사진을 골라서 대회에서 사용되고 있다. 전체 사진으로 보았을 때는 약 120만 개의 train dataset과 5만 개의 validation dataset, 15만 개의 test dataset을 제공한다. 제공되는 데이터셋의 사진들은 다양한 크기의 사진들이 존재했는데, AlexNet은 고정된 크기의 사진이 필요함에 따라 다음 과정을 거쳤다. 

* 이미지의 가로 세로중 짧은 쪽의 픽셀 수가 256이 되도록 줄인다.
* 256 x 256 의 정사각형 이미지를 얻기 위해 줄인 사진의 중앙을 crop 하여 얻어낸다.
* 각 픽셀의 RGB값을 전체 이미지의 RGB 평균으로 빼준다. 

# Architecture
alexnet의 구조를 알고 어떤 기법이 사용되었는지 소개한다. 각 기법들을 소개하는 순서는 중요도에 의해 정렬되었으며, 가장 처음에 나오는 기법이 가장 중요한것이다.

## 1. ReLU Nonlinearilty
일반적으로 뉴런의 결과를 낼때 활성함수로 하이퍼볼릭 탄젠트나 시그모이드 함수를 이용한다. 하지만 이런 saturating nonlinearities를 사용하는 것보다 non-saturating nonlinearities인 ReLU를 사용하느 것이 학습속도 측면에서 훨씬 빠르다고 주장한다. 

※ saturating nonlinearity
: 입력 x가 무한대로 갈때 함수의 값이 어떤 범위내에서만 움직이는 것

ex) 하이퍼볼릭 탄젠트, 시그모이드 함수
![hyper](/assets/img/favicons/hyperbolic%20tangent.png)
![sigmoid](/assets/img/favicons/igmoid%20activation%20function.png)

※ non-saturating nonlinearity
: 입력 x가 무한대로 갈때 함수의 값이 무한대로 가는 것

ex) ReLU
![relu](/assets/img/favicons/relu.png)

![satu](/assets/img/favicons/satu.png)

위 사진을 보면 점선(saturating nonlinearity)인 하이퍼볼릭 탄젠트 함수를 사용하는 것보다 실선(non-saturating nonlinearity)인 ReLU를 사용하는 것이 학습에러율 25% 이하로 낮추는데 걸리는 시간이 6배 단축된다고 한다. 또한 ReLU를 사용하지 않았다면 엄청난 크기의 신경망을 학습하지 못했을 것이라고 발표하며 ReLU의 사용을 강조하였다.

물론 non-saturating 활성함수의 사용은 AlesNet이 처음은 아니였다. 이전에 |tanh(x)|와 같은 함수를 사용한 논문이 있었으며 위의 함수를 통해 오버피팅을 방지했다는 발표가 있었다. 하지만, AlexNet이 필요로 한것은 빠른 학습속도이었기에 다른 함수를 필요로 했던 것이다.

## 2. Multiple GPUs
저자들이 학습에서 사용한 GPU는 GTX 580 GPU 로 3GB의 메모리를 가지고 있는 장비이다. 학습 데이터셋으로 주어지는 120만장의 이미지는 모델을 학습시키지에 충분한 양이었지만, 그 개수와 크기는 GPU가 감당할 수 없었다. 따라서 저자들은 2개의 GPU를 병렬적으로 사용하여 문제를 해결했다. 

다만, 딥러닝의 특정 레이어에서만 2개의 GPU가 서로 데이터를 교환할 수 있게 만들고 나머지 레이어에서는 같은 GPU로부터 연산을 이어받을 수 있게 만들었다. 각각의 GPU는 커널을 절반으로 나누어서 연산을 처리하게 하여 하나에 가해지는 부담을 줄였다. 이 과정은 아래 AlexNet의 구조를 도식화한 부분에서 묘사되어있다.

## 3. Local Response Normalization
현재는 Batch Nomalization의 등장으로 잘 사용하지 않는 nomalization기법이지만, 당시에는 ReLU의 결과값이 너무 커져서 주변 뉴런에 영향을 주는 것을 방지하기 위해 nomalization기법이 필요했다. CNN특성상 현재 픽셀에 대한 결과를 계산하기 위해 아웃한 픽셀도 참고하기 때문에 영향을 주는 것이다.

![equ](/assets/img/favicons/equation.png)

* \[a^i_{x,y}\] : (x,y)에 존재하는 픽셀에 대해 i번째 커널을 적용하여 얻은 결과에 

---