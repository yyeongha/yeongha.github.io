---
title: 딥러닝 용어 정리
categories: [용어] 
date: 2024-05-29
last_modified_at: 2024-06-03
---
## A

## B

## C
#### * Convergence rate (수렴속도) 
: 인공지능 모델 학습에서 목표하는 성능(최적의 모델)에 얼마나 빨리 도달하는지를 나타내는 지표이다. \
-> Exponentially low convergence rate  
: 깊은 plain 모델은 수렴 속도가 지수적으로 감소하는 문제가 있다. 즉, 층이 깊어질수록 학습 속도가 매우 느려져서 좋은 성능을 내기 어렵다.


## D
#### * depthwise convolution 
: group convolution의 특수한 경우로, 각 채널별로 독립적인 convolution을 수행한다. 이는 모바일 네트워크(MobileNet)와 같은 경량화된 네트워크에서도 사용된다.

![depth](https://github.com/yyeongha/yyeongha.github.io/blob/main/assets/img/favicons/2024-05-29-word-img/depth.png?raw=true)

#### * downsampling 
: 신호처리에서 말하는 용어로 sample의 개수를 줄이는 처리과정을 의미한다. \
ex) pooling, Dilated (Atrous) convolution, Depthwise convolution

![pooling](https://github.com/yyeongha/yyeongha.github.io/blob/main/assets/img/favicons/2024-05-29-word-img/pooling.png?raw=true)

#### * disparity

#### * degradation 
: 딥러닝 모델은 레이어를 깊게 쌓을수록 성능이 좋아질 여지가 생기지만, 문제는 레이어를 깊게 쌓을수록 기울기 소실(gradient vanishing)과 기울기 폭발(gradient exploding)이 쉽게 나타나서 모델을 학습시키기 쉽지 않다. 이렇게 레이어가 깊어졌음에도 성능은 오히려 하락하는 문제를 degradation이라고 부릅니다.


#### * dropout
: 우리는 인물 사진에서 사람의 손이 가려져 있더라도 여전히 사진 속의 대상이 사람이라는 것을 알 수 있다. 그러나 학습데이터의 모든 인물 사진은 사람의 손이 보이는 이미지이고, 이 학습 데이터에 지나치게 과적합된 딥러닝 모델이라면 손이 안보이는 인물사진을 모델에게 줄 경우, 손이 안보인다는 이유로 모델은 이 사진이 사람이 아니라고 판정할 수도 있다.
dropout은 

## E
## F
#### * FLOP(FLoating-point OPerations) 
: 컴퓨터가 수행하는 부동 소수점 연산 횟수를 의미합니다. 딥러닝 모델의 복잡도를 나타내는 지표로 널리 사용되며, 모델의 학습 및 추론에 필요한 계산량을 추정하는 데 활용됩니다. FLOP이 높을수록 모델이 더 복잡하고 많은 계산이 필요하다는 것을 의미합니다. \
쉽게 말해, FLOP은 모델이 얼마나 많은 계산을 해야 하는지를 나타내는 숫자라고 생각하면 됩니다. 예를 들어, 덧셈, 뺄셈, 곱셈, 나눗셈과 같은 연산을 몇 번 해야 하는지 세는 것이죠. 딥러닝 모델은 수많은 연산을 수행하므로 FLOP은 모델의 크기나 성능을 비교할 때 중요한 기준이 됩니다. 

![figure2](https://github.com/yyeongha/yyeongha.github.io/blob/main/assets/img/favicons/2024-05-29-word-img/figure2.png?raw=true)

#### Feed-forward neural networks (FFNN, 순방향 신경망) 
: 입력값이 출력까지 한 방향으로 전달되는 구조를 가진 인공 신경망이다. 이 구조 때문에 'feedforward'라는 이름이 붙었으며, 데이터가 신경망의 입력층에서 출력층까지 단방향(Input Layer -> Hidden Layer -> Output Layer)으로 이동하므로 순환(loop) 또는 피드백이 없다. 이러한 특징으로 인해 FNN은 시계열 데이터와 같은 연속적인 정보를 처리하는 데 한계가 있다.

![ffnn]()


## G
#### * group convolution


## H
#### * Highway Networks 
: 신경망의 깊이가 깊어질수록 정보가 왜곡되거나 사라져 학습이 어려워지는 문제가 발생한다.
1) 정보 고속도로 건설 \
Highway Networks는 이 문제를 해결하기 위해 신경망 층 사이에 "정보 고속도로"를 건설한다. 이 고속도로는 정보가 깊은 층까지 빠르고 안전하게 전달될 수 있도록 돕는다. 마치 깊은 숲 속에 고속도로를 뚫어 길을 잃지 않고 목적지까지 쉽게 도달하는 것과 같다.
2) gate unit : 교통 정리 전문가 \
고속도로에는 "게이트 유닛"이라는 교통 정리 전문가가 있다. 게이트 유닛은 각 층에서 어떤 정보를 고속도로로 보낼지, 어떤 정보를 일반 도로로 보낼지 판단한다. 이를 통해 불필요한 정보는 걸러내고 중요한 정보만 효율적으로 전달하여 학습 속도를 높인다.

![highway]()


## I
#### * Inductive Bias (귀납 편향) 
: 쉽게 말해, 우리가 어떤 문제를 해결할 때 특정 방식을 선호하는 경향이다. 예를 들어, 탐정이 사건을 해결할 때 범인은 주변 인물일 가능성이 높다고 생각하는 것이 귀납편향이다.\
기계 학습 모델도 마찬가지다. 모델은 주어진 데이터를 보고 문제를 해결하는 방법을 배우는데, 이때 특정 패턴이나 규칙을 더 선호하는 경향을 가진다. 

#### * Inverted Bottleneck 
: MobileNetV2에서 처음 도입된 개념으로, 전통적인 Bottleneck 구조와 반대로 중간층의 차원을 확장하는 방식을 사용한다. 이를 통해 더 많은 특징을 학습할 수 있으며, 네트워크의 표현력을 향상시키는데 도움을 준다.

#### * Identity Mapping 
: 입력값을 아무런 변형 없이 그대로 출력하는 것을 의미한다. ResNet에서는 이러한 Identity Mapping을 통해 신경망의 일부 층을 건너뛰는 "지름길(Shortcut Connection)"을 만들어준다.

![resuvplain](https://github.com/yyeongha/yyeongha.github.io/blob/main/assets/img/favicons/2024-06-03-resnet-img/resuvplain.png?raw=true)


## J
## K
## L


## M
#### * MLP (Multi-Layer Perceptron, 다층 퍼셉트론)


## N  
#### * normalization(정규화) 
: 값의 범위(scale)를 0~1 사이의 값으로 바꾸는 것. 
비유를 들어 설명하자면, 인공지능 모델 학습은 여러 선수가 참여하는 팀 스포츠와 같다. 각 선수(특성)는 서로 다른 능력치를 가지고 있으며, 팀의 승리(모델 성능)를 위해서는 모든 선수의 능력치가 균형을 이루어야 한다. 만약, 특정 선수의 능력치가 다른 선수들에 비해 월등히 높다면, 그 선수에게만 의존하게 되어 팀 전체의 균형이 무너질 수 있다.
정규화는 데이터의 특성(feature) 값을 일정한 범위로 조정하여 모든 특성이 동등한 중요도를 가지도록 만드는 과정입니다. 이를 통해 모델 학습 과정에서 특정 특성이 과도하게 영향을 미치는 것을 방지하고, 모델의 성능을 향상시킬 수 있습니다.

* 장점
    * 학습 속도 향상: 특성 값의 범위가 비슷해지면 경사 하강법과 같은 최적화 알고리즘이 더 빠르게 수렴할 수 있다.
    * 모델 성능 향상: 특정 특성이 과도하게 영향을 미치는 것을 방지하여 모델의 일반화 성능을 높일 수 있다.
* 단점
    * 데이터 손실 가능성: 정규화 과정에서 일부 정보가 손실될 수 있습니다.
    * 계산 비용 증가: 정규화를 위해 추가적인 계산이 필요합니다.

![normalization](https://github.com/yyeongha/yyeongha.github.io/blob/main/assets/img/favicons/2024-06-03-resnet-img/normalization.png?raw=true)

## O
## P
#### * Pooling 
: 특정한 규칙(Max, Average)에 의해서 Kernel 내에서 값을 만들어 내거나 추출하는 방법이다.
![]()

#### * Pre-conditioning 
: 본격적인 학습에 앞서 데이터 또는 모델을 변환하여 학습 과정을 더 쉽고 빠르게 만드는 것을 의미한다. 예를 들어, 산을 오르기 전에 등산화 끈을 단단히 조이고, 지팡이를 준비하는 것과 같다. \
-> ResNet에서는 Identity Mapping과 Residual Block을 통해 계층 기반 Pre-conditioning을 구현한다. Identity Mapping은 입력값을 변형 없이 다음 층으로 전달하여 정보 손실을 방지하고, Residual Block은 입력값과 출력값의 차이(잔차)를 학습하여 학습 신호를 강화한다.


## Q
## R
#### * Residual learning 
: 다음 그림처럼 convolutional layer의 input에 해당하는 x값을 convolutional layer의 output에 다시 더해줌으로써, convolutional layer가 input 값과 output 사이의 차이(잔차, residual)를 학습하는데만 집중할 수 있도록 하는 기법

![residual](https://github.com/yyeongha/yyeongha.github.io/blob/main/assets/img/favicons/2024-06-03-resnet-img/residual.png?raw=true)

#### * Residual Block
: residual learning을 하도록 구성된 하나의 단위 

![resuvplain](https://github.com/yyeongha/yyeongha.github.io/blob/main/assets/img/favicons/2024-06-03-resnet-img/resuvplain.png?raw=true)
![resplain](https://github.com/yyeongha/yyeongha.github.io/blob/main/assets/img/favicons/2024-06-03-resnet-img/resplain.png?raw=true)

-> Residual Block을 plain layer과 비교하여 설명하면, plain layer는 동일한 연산(f(x))을 수행하고 난 후 input x를 더해주지 않지만, \
Residual Block에서는 동일한 연산(f(x))을 수행하고 난 후 input x를 더해준다. 입력값 x를 F(x) + x 형태로 변환하는 블록이다. \
즉, plain layer와 다르게 Residual Block에는 skip connection이 존재한다. 

#### * Residual Function (F(x)) 
: 입력값과 출력값의 차이를 나타내는 함수


## S
#### * Stochastic Gradient Descent(SGD, 확률적 경사하강법) 
: 전체 data를 가지고 한번의 loss function을 계산하는게 아니라 batch단위로 loss function을 계산. 이로인해 loss function을 여러번 빨리 계산할 수 있다. 

![sgd](https://github.com/yyeongha/yyeongha.github.io/blob/main/assets/img/favicons/2024-06-03-resnet-img/sgd.png?raw=true)

#### * swin
* ViT가 classificatoin이외에서는 좋은 성과가 없자 이를 개선하고자 계층적 구조를 도입함
* ViT와 마찬가지로 patch(4x4) 단위를 씀
* global attention과 local attention을 모두 도입하여 ViT의 큰 연산량보다 최적화된 연산량을 보임

![swin](https://github.com/yyeongha/yyeongha.github.io/blob/main/assets/img/favicons/2024-05-29-word-img/swin.png?raw=true)

#### * self attention 
: 자연어 처리 분야에서 널리 사용되는 어텐션 메커니즘의 한 종류이다. \
기존의 어텐션 메커니즘은 입력 시퀀스의 각 요소가 다른 시퀀스의 요소들과 어떻게 관련되는지 파악하는 데 중점을 두었다면, self-attention은 입력 시퀀스 내의 각 요소가 같은 시퀀스 내의 다른 요소들과 어떻게 관련되는지 파악하는 데 초점을 맞춘다. 그 이유는 바로 문장에서의 단어들의 연관성을 알기위해서이다.

![attention](https://github.com/yyeongha/yyeongha.github.io/blob/main/assets/img/favicons/2024-05-29-word-img/attention.png?raw=true)
![selfattention](https://github.com/yyeongha/yyeongha.github.io/blob/main/assets/img/favicons/2024-05-29-word-img/selfattention.png?raw=true)

#### * shortcut connection (skip connection) 
: residual learning을 위해 input 값을 output 값에 더하는 과정 \
즉, 하나의 layer의 output을 몇개의 layer를 건너뛰고 다음 layer의 input에 추가되는 것을 의미 \
skip connection을 사용하게 되면 각각의 layer가 작은 정보들을 추가적으로 학습하도록 한다. 

* 장점
    * 파라미터 추가 없음 : Shortcut Connection은 단순히 이전 레이어의 출력값을 현재 레이어의 출력값에 더해주는 역할을 한다. 이 과정에서 새로운 파라미터가 추가되지 않기 때문에, 모델의 복잡성을 증가시키지 않는다.
    * 절대 닫히지 않음 (Zero-Gating 문제 해결) : 일부 신경망 구조에서는 특정 조건에 따라 일부 연결이 닫히는 현상(Zero-Gating)이 발생할 수 있다. 이는 정보 흐름을 방해하고 학습을 어렵게 만든다. 하지만, ResNet의 Shortcut Connection은 항상 열려있기 때문에 이러한 문제가 발생하지 않는다.


## T
## U
#### * Unreferenced mapping(참조되지 않은 매핑) 
: 기존의 신경망 층들이 수행하는 입력값을 출력값으로 변환하는 일반적인 함수를 의미한다. 즉, 입력값에 대한 어떠한 참조 없이 독립적으로 출력값을 계산하는 방식이다.

<-> residual mapping(잔차 매핑) : 입력값에 대한 참조를 포함하는 함수


## V
#### * ViT (vision transformer)
* ViT에서는 입력이미지를 자연어처리의 token과 같이 patch 단위로 나누어 사용하는 것이 핵심. 밑의 사진에서 보이는 patch가 자연어 처리에서의 token과 같다.
* inductive bias(귀납편향)가 없어 classification에서는 대용량 데이터에서 ConvNet보다 유리한 면을 보임
* NLP와 같이 patch 단위(16x16)로 이미지를 쪼개서 사용함

![vit](https://github.com/yyeongha/yyeongha.github.io/blob/main/assets/img/favicons/2024-05-29-word-img/vit.png?raw=true)
![vit2](https://github.com/yyeongha/yyeongha.github.io/blob/main/assets/img/favicons/2024-05-29-word-img/vit2.png?raw=true)

#### * Vanishing Gradient (Exploding Gradient, 기울기 소실) 
: 비유를 통해 쉽게 말하자면 첩첩산중 깊은 골짜기에서 길을 잃었다고 상상해 보자. 산 정상으로 올라가려면, 주변 지형을 살펴 경사가 가장 가파른 곳으로 이동해야 한다. 이때, 경사는 우리가 나아가야 할 방향을 알려주는 중요한 정보이다. \
인공지능 모델 학습도 이와 비슷하다. 모델은 데이터를 통해 학습하며, 더 좋은 결과를 내기 위해 끊임없이 자신을 수정한다. 이때, '경사'는 모델이 어떤 방향으로 수정되어야 할지 알려주는 정보이다. \
즉, 심층 신경망이 학습을 하는 과정 중, 역전파(Backpropagation) 알고리즘이 작동하면서 각 층에 대한 가중치를 갱신하는데, 이 때 Gradient가 매우 작아져서 실질적으로 가중치가 갱신되지 않는 현상이 발생한다. 그 결과 신경망의 앞부분 층은 학습이 거의 이루어지지 않게 된다.

![vanishing](https://github.com/yyeongha/yyeongha.github.io/blob/main/assets/img/favicons/2024-06-03-resnet-img/vanishing.png?raw=true)
ex) Simoid, tanh 함수 

-> 해결방법 : ReLU, Residual connection, Gradient Clipping, Weight initialization

#### * Vector Quantization (VQ, 벡터 양자화) 
: 벡터 양자화는 데이터 압축에 사용되는 기술로, 많은 데이터를 비슷한 특징을 가진 몇 개의 대표값으로 줄여서 표현하는 방법이다. \
비유를 통해 쉽게 말하자면, 다양한 색깔의 물감이 가득한 팔레트를 상상해 보자. 벡터 양자화는 이 팔레트에서 가장 많이 사용되는 몇 가지 색깔만 선택하여 그림을 그리는 것과 같다. 그림의 디테일은 다소 떨어질 수 있지만, 전체적인 색감은 유지하면서 데이터 용량을 줄일 수 있다. \

![vq](https://github.com/yyeongha/yyeongha.github.io/blob/main/assets/img/favicons/2024-06-03-resnet-img/vq.png?raw=true)

#### * Vector of Locally Aggregated Descriptors (VLAD) 
: 이미지를 숫자로 표현하는 방법 중 하나이다. 이 숫자들은 이미지의 특징을 담고 있어서, 비슷한 이미지끼리 비슷한 숫자를 가지게 된다. \
VLAD의 실행과정을 비유를 들어 설명하자면,
1. Extracting Local Descriptions \
: 먼저 이미지를 작은 퍼즐 조각처럼 여러 부분으로 나눈다. 각 퍼즐 조각은 색깔, 무늬, 모양 등의 특징을 가지고 있다.
2. Codebook Generation \
: 비슷한 퍼즐 조각끼리 모아서 여러 그룹으로 분류한다. 예를 들어, 파란색 하늘 조각은 한 그룹, 녹색 잎사귀 조각은 다른 그룹으로 모을 수 있다.
3. Description - Codebook Accumulation \
: 각 그룹에서 가장 대표적인 퍼즐 조각을 하나씩 정한다. 이 대표 퍼즐 조각은 해당 그룹의 특징을 가장 잘 나타내는 조각이다.
4. L2 Normalization \
: 각 그룹에 속한 퍼즐 조각들의 비교 결과 숫자를 모두 합쳐서 하나의 숫자 목록을 만든다. 이 숫자 목록이 바로 VLAD이다.



## W
## X

## Y


## Z
#### * Zero Mapping 
: 입력값을 모두 0으로 매핑하는 함수를 의미한다. 즉, 입력값에 관계없이 출력값이 항상 0이 되는 함수이다. \
ResNet 논문에서는 Residual Block의 학습 난이도를 설명하기 위해 Zero Mapping을 언급한다. Residual Block은 입력값 x를 F(x) + x 형태로 변환하는데, 이때 F(x)는 Residual Function(잔차 함수)이다. \
만약 F(x)가 Zero Mapping이라면, Residual Block의 출력값은 입력값 x와 동일하게 된다. 즉, 입력값을 변경하지 않고 다음 층으로 전달하는 Identity Mapping과 동일한 효과를 얻을 수 있다.




---