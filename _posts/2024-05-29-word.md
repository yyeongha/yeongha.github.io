---
title: 딥러닝 용어 정리
categories: [용어] 
date: 2024-05-29
last_modified_at: 2024-05-29
---
# 딥러닝 용어 정리 (A-Z)

## A
## B
## C

## D
※ depthwise convolution
: group convolution의 특수한 경우로, 각 채널별로 독립적인 convolution을 수행한다. 이는 모바일 네트워크(MobileNet)와 같은 경량화된 네트워크에서도 사용된다.

![depth](https://github.com/yyeongha/yyeongha.github.io/blob/main/assets/img/favicons/2024-05-15-img/depth.png?raw=true)

※ downsampling

## E
## F
※ FLOPs


## G
※ group convolution


## H

## I
※ Inductive Bias (귀납 편향)
: 쉽게 말해, 우리가 어떤 문제를 해결할 때 특정 방식을 선호하는 경향이다. 예를 들어, 탐정이 사건을 해결할 때 범인은 주변 인물일 가능성이 높다고 생각하는 것이 귀납편향이다.\
기계 학습 모델도 마찬가지다. 모델은 주어진 데이터를 보고 문제를 해결하는 방법을 배우는데, 이때 특정 패턴이나 규칙을 더 선호하는 경향을 가진다. 

※ Inverted Bottleneck
: MobileNetV2에서 처음 도입된 개념으로, 전통적인 Bottleneck 구조와 반대로 중간층의 차원을 확장하는 방식을 사용한다. 이를 통해 더 많은 특징을 학습할 수 있으며, 네트워크의 표현력을 향상시키는데 도움을 준다.

## J
## K
## L

## M
※ MLP (Multi-Layer Perceptron, 다층 퍼셉트론)


## N  
## O
## P
## Q
## R
※ Residual Block


## S
※ swin
* ViT가 classificatoin이외에서는 좋은 성과가 없자 이를 개선하고자 계층적 구조를 도입함
* ViT와 마찬가지로 patch(4x4) 단위를 씀
* global attention과 local attention을 모두 도입하여 ViT의 큰 연산량보다 최적화된 연산량을 보임

![swin](https://github.com/yyeongha/yyeongha.github.io/blob/main/assets/img/favicons/2024-05-15-img/swin.png?raw=true)


※ self attention
: 자연어 처리 분야에서 널리 사용되는 어텐션 메커니즘의 한 종류이다. \
기존의 어텐션 메커니즘은 입력 시퀀스의 각 요소가 다른 시퀀스의 요소들과 어떻게 관련되는지 파악하는 데 중점을 두었다면, self-attention은 입력 시퀀스 내의 각 요소가 같은 시퀀스 내의 다른 요소들과 어떻게 관련되는지 파악하는 데 초점을 맞춘다. 그 이유는 바로 문장에서의 단어들의 연관성을 알기위해서이다.

![attention]()
![selfattention]()


## T
## U

## V
※ ViT (vision transformer)
* ViT에서는 입력이미지를 자연어처리의 token과 같이 patch 단위로 나누어 사용하는 것이 핵심. 밑의 사진에서 보이는 patch가 자연어 처리에서의 token과 같다.
* inductive bias(귀납편향)가 없어 classification에서는 대용량 데이터에서 ConvNet보다 유리한 면을 보임
* NLP와 같이 patch 단위(16x16)로 이미지를 쪼개서 사용함

![vit](https://github.com/yyeongha/yyeongha.github.io/blob/main/assets/img/favicons/2024-05-15-img/vit.png?raw=true)
![vit2](https://github.com/yyeongha/yyeongha.github.io/blob/main/assets/img/favicons/2024-05-15-img/vit2.png?raw=true)

## W
## X
## Y
## Z






---