---
title: 딥러닝 용어 정리
categories: [용어] 
date: 2024-05-29
last_modified_at: 2024-05-29
---
## A
## B
## C

## D
※ depthwise convolution
: group convolution의 특수한 경우로, 각 채널별로 독립적인 convolution을 수행한다. 이는 모바일 네트워크(MobileNet)와 같은 경량화된 네트워크에서도 사용된다.

![depth](https://github.com/yyeongha/yyeongha.github.io/blob/main/assets/img/favicons/2024-05-29-word-img/depth.png?raw=true)

※ downsampling
:신호처리에서 말하는 용어로 sample의 개수를 줄이는 처리과정을 의미한다.
ex) pooling, Dilated (Atrous) convolution, Depthwise convolution


## E
## F
※ FLOP(FLoating-point OPerations)
: 컴퓨터가 수행하는 부동 소수점 연산 횟수를 의미합니다. 딥러닝 모델의 복잡도를 나타내는 지표로 널리 사용되며, 모델의 학습 및 추론에 필요한 계산량을 추정하는 데 활용됩니다. FLOP이 높을수록 모델이 더 복잡하고 많은 계산이 필요하다는 것을 의미합니다. \
쉽게 말해, FLOP은 모델이 얼마나 많은 계산을 해야 하는지를 나타내는 숫자라고 생각하면 됩니다. 예를 들어, 덧셈, 뺄셈, 곱셈, 나눗셈과 같은 연산을 몇 번 해야 하는지 세는 것이죠. 딥러닝 모델은 수많은 연산을 수행하므로 FLOP은 모델의 크기나 성능을 비교할 때 중요한 기준이 됩니다. 

![figure2](https://github.com/yyeongha/yyeongha.github.io/blob/main/assets/img/favicons/2024-05-29-word-img/figure2.png?raw=true)


## G
※ group convolution


## H

## I
※ Inductive Bias (귀납 편향)
: 쉽게 말해, 우리가 어떤 문제를 해결할 때 특정 방식을 선호하는 경향이다. 예를 들어, 탐정이 사건을 해결할 때 범인은 주변 인물일 가능성이 높다고 생각하는 것이 귀납편향이다.\
기계 학습 모델도 마찬가지다. 모델은 주어진 데이터를 보고 문제를 해결하는 방법을 배우는데, 이때 특정 패턴이나 규칙을 더 선호하는 경향을 가진다. 

※ Inverted Bottleneck
: MobileNetV2에서 처음 도입된 개념으로, 전통적인 Bottleneck 구조와 반대로 중간층의 차원을 확장하는 방식을 사용한다. 이를 통해 더 많은 특징을 학습할 수 있으며, 네트워크의 표현력을 향상시키는데 도움을 준다.

## J
## K
## L

## M
※ MLP (Multi-Layer Perceptron, 다층 퍼셉트론)


## N  
## O
## P
※ Pooling
: 특정한 규칙(Max, Average)에 의해서 Kernel 내에서 값을 만들어 내거나 추출하는 방법이다.
![]()



## Q
## R
※ Residual Block


## S
※ swin
* ViT가 classificatoin이외에서는 좋은 성과가 없자 이를 개선하고자 계층적 구조를 도입함
* ViT와 마찬가지로 patch(4x4) 단위를 씀
* global attention과 local attention을 모두 도입하여 ViT의 큰 연산량보다 최적화된 연산량을 보임

![swin](https://github.com/yyeongha/yyeongha.github.io/blob/main/assets/img/favicons/2024-05-29-word-img/swin.png?raw=true)


※ self attention
: 자연어 처리 분야에서 널리 사용되는 어텐션 메커니즘의 한 종류이다. \
기존의 어텐션 메커니즘은 입력 시퀀스의 각 요소가 다른 시퀀스의 요소들과 어떻게 관련되는지 파악하는 데 중점을 두었다면, self-attention은 입력 시퀀스 내의 각 요소가 같은 시퀀스 내의 다른 요소들과 어떻게 관련되는지 파악하는 데 초점을 맞춘다. 그 이유는 바로 문장에서의 단어들의 연관성을 알기위해서이다.

![attention]()
![selfattention]()


## T
## U

## V
※ ViT (vision transformer)
* ViT에서는 입력이미지를 자연어처리의 token과 같이 patch 단위로 나누어 사용하는 것이 핵심. 밑의 사진에서 보이는 patch가 자연어 처리에서의 token과 같다.
* inductive bias(귀납편향)가 없어 classification에서는 대용량 데이터에서 ConvNet보다 유리한 면을 보임
* NLP와 같이 patch 단위(16x16)로 이미지를 쪼개서 사용함

![vit](https://github.com/yyeongha/yyeongha.github.io/blob/main/assets/img/favicons/2024-05-29-word-img/vit.png?raw=true)
![vit2](https://github.com/yyeongha/yyeongha.github.io/blob/main/assets/img/favicons/2024-05-29-word-img/vit2.png?raw=true)

## W
## X
## Y
## Z






---