---
title: 논문리뷰_A ConvNet for the 2020s
categories: [논문리뷰] 
date: 2024-05-15
last_modified_at: 2024-05-15
---

논문 출처 : [A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545v2)

2020년도에 ViT(Vision Transofrmer)가 발표된 이후 Vision task에서 Transformer에 연구가 집중되고 있지만 CNN에 Transformer 구조 및 최신 기법들을 적용한 ConvNeXt라는 모델을 제안하고 있으며 높은 성능을 통해 CNN이 여전히 강하다는 것을 주장하는 논문이다.


# 1. Introduction
2010년대 컴퓨터 비전 분야는 이미지 인식에 탁월한 ConvNets의 발전으로 급성장했다. 

자연어 처리 분야의 ViT(Vision Transformer)가 컴퓨터 비전에 도입되면서 새로운 변화를 맞이했다. ViT는 이미지 분류에는 뛰어났지만 객체 감지나 이미지 분할과 같은 작업에는 한계를 보였다. 

이후 Swin Transformer와 같은 계층적 트랜스포머 모델이 등장하여 ConvNets의 장점을 일부 차용하며 다양한 컴퓨터 비전 작업에서 좋은 성능을 보였다. 

본 논문에서는 ConvNets를 개선하여 트랜스포머 모델만큼의 성능을 내는 ConvNeXt라는 새로운 모델을 제안한다. ConvNeXt는 다양한 벤치마크에서 Swin Transformer와 비슷하거나 더 나은 성능을 보였고, 기존 ConvNets의 단순함과 효율성도 유지했다. 본 연구는 ConvNets가 여전히 컴퓨터 비전 분야에서 중요한 역할을 할 수 있음을 보여주며, 컴퓨터 비전 모델 설계에 새로운 시각을 제시한다. 

![figure1](https://github.com/yyeongha/yyeongha.github.io/blob/main/assets/img/favicons/2024-05-15-img/figure1.png?raw=true) 

중간 선을 기준으로 ImageNet-1K, ImageNet-22K 데이터를 통해서 학습, 사전학습한 모델의 top1 accuracy이다.

각 버블의 면적은 모델 제품군의 변형 FLOP(연산량)에 비례한다. 


※ ViT (vision transformer)
* ViT에서는 입력이미지를 자연어처리의 token과 같이 patch 단위로 나누어 사용하는 것이 핵심. 밑의 사진에서 보이는 patch가 자연어 처리에서의 token과 같다.
* inductive bias가 없어 classification에서는 대용량 데이터에서 ConvNet보다 유리한 면을 보임
* NLP와 같이 patch 단위(16x16)로 이미지를 쪼개서 사용함

![vit](https://github.com/yyeongha/yyeongha.github.io/blob/main/assets/img/favicons/2024-05-15-img/vit.png?raw=true)
![vit2](https://github.com/yyeongha/yyeongha.github.io/blob/main/assets/img/favicons/2024-05-15-img/vit2.png?raw=true)

※ swin
* ViT가 classificatoin이외에서는 좋은 성과가 없자 이를 개선하고자 계층적 구조를 도입함
* ViT와 마찬가지로 patch(4x4) 단위를 씀
* global attention과 local attention을 모두 도입하여 ViT의 큰 연산량보다 최적화된 연산량을 보임

![swin](https://github.com/yyeongha/yyeongha.github.io/blob/main/assets/img/favicons/2024-05-15-img/swin.png?raw=true)



# 2. Modernizing a ConvNet: a Roadmap
본 논문에서는 ResNet-50/200 모델을 시작으로 단계별로 모델 구조를 변경하며 성능을 개선하는 과정을 거쳐, 최종적으로 ConvNeXt라는 새로운 ConvNet 모델을 제안한다. 각 단계는 다음과 같다.

1) ResNet-50/200 : resnet 50/200을 베이스로 잡아 transformer의 학습기법을 도입
* epoch을 90 -> 300으로 증가
* AdamW optimizer 사용
* data augmentation(Mixup, Cutmix, RandAugment, Random Erasing) 추가
* Regularization(Stochastic Depth, Label Smoothing.)
-> accuracy : 76.1% -> 78.8% (+2.7%)

이를 통해 전통적인 ConvNet과 ViT의 성능 차이의 많은 부분이 training technques에서 기인하였음을 알 수 있다. 

2) macro design
* Changing stage compute ratio
ResNet-50의 경우 stage 마다 블록의 갯수가 3,4,6,3개 이렇게 들어있는데, swin-T의 경우 1:1:3:1로 들어있다. 그래서 ResNet에도 이 비율을 맞춰 3,3,9,3개로 바꾸었다..  
-> accuracy : 78.8% -> 79.4% (+0.6%)

![macro](https://github.com/yyeongha/yyeongha.github.io/blob/main/assets/img/favicons/2024-05-15-img/macro.png?raw=true)

* Changing stem to “Patchify”
기존 모델들의 stem부분을 보면
  * ResNet : stride2, 7x7 conv layer,max pool을 통해 이미지를 4x4로 downsampling
  * ViT : “Patchify”라는 전략을 이용한다. 이미지를 작은 패치로 나누어 각각을 개별 입력 토큰으로 취급하는데, 이 과정에서는 보통 14x14 또는 16x16 크기의 큰 커널과 비중첩(non-overlapping) 컨볼루션을 사용하여 패치를 생성한다.
    * Swin Transformer : 더 작은 4x4형태의 non-overlapping convolution을 사용하여 패치 생성

논문에서는 ResNet을 베이스로 하여 Swin Transformer 방식을 차용하여 stem cell을 4x4 커널 크기의 non-overlapping convolution 레이어를 사용하는 레이어로 대체했다.

-> accuracy : 79.4% -> 79.5% (+0.1%)

![patchify](https://github.com/yyeongha/yyeongha.github.io/blob/main/assets/img/favicons/2024-05-15-img/patchify.png?raw=true)


3) ResNeXt
* group convolution 도입
  * 기본 ResNet 구조에서 3x3 convolution layer를 group convolution으로 대체한다. 이는 각 컨볼루션 필터가 입력 채널을 여러 그룹으로 나누어 병렬로 적용된다.
  * group convolution을 사용함으로써, 필터 수를 증가시키지 않으면서도 모델의 용량을 확장할 수 있다. 

  ![resnext]()

* depthwise convolution 사용

  ![depth]()

* 네트워크 width 증가
  * depthwise convolution으로 인해 FLOPs이 감소하기 때문에, 이를 보완하기 위해 네트워크의 폭을 증가시킨다. 이는 모델의 용량을 증가시켜 성능을 향상시키는 역할을 한다.
-> accuracy : 79.5% -> 80.5 (+1.0%)

※ depthwise convolution
: group convolution의 특수한 경우로, 각 채널별로 독립적인 convolution을 수행한다. 이는 모바일 네트워크(MobileNet)와 같은 경량화된 네트워크에서도 사용된다.


4) Inverted Bottleneck
* 차원확장
  * 확장 비율 : MLP 블록의 숨겨진 차원을 입력 차원의 4배로 확장한다. 
  * FLOPs 감소 : 깊이별 convolution layer의 FLOPs가 증가함에도 불구하고, 다운샘플링 잔여 블록의 1x1 convolution layer에서 FLOPs를 크게 줄일 수 있다.
-> accuracy : 80.5% -> 80.6% (+0.1%)

![figure3]()
(a) ResNeXt (Bottleneck 구조) : 1x1 conv로 채널을 줄인 후 3x3 conv를 진행하고 다시 1x1으로 채널을 키우는 구조
(b) Inverted Bottleneck 
(c) Spatial Depthwise Conv layer 위치 이동

※ Inverted Bottleneck
: MobileNetV2에서 처음 도입된 개념으로, 전통적인 Bottleneck 구조와 반대로 중간층의 차원을 확장하는 방식을 사용한다. 이를 통해 더 많은 특징을 학습할 수 있으며, 네트워크의 표현력을 향상시키는데 도움을 준다.


5) 큰 커널 크기
전통적인 ConvNet
* transformer에서 global receptive field를 가질 수 있게 해주는 non local self attention에 주목
* VGG이후로 대중화된 3x3사이즈가 이나라 7x7사이즈를 사용
* deptwise conv layer를 순서상 위로 올려 더 넓은 kernel size를 탐색할 수 있도록 함
-> accuracy : 76.1% -> 78.8% (+2.7%)

6) micro design
* BERT와 GPT-2이후 대부분의 Transformer에서는 ReLU보다 GELU를 채택하기 때문에 convnext 또한 GELU로 변경
* 활성화 함수 및 정규화 레이어의 개수를 줄임
* Batch Normalization(BN)을 Layer Normalization(LN)으로 변경
-> accuracy : 80.6% -> 81.5% (+0.9%)

* swin에서의 구조를 따와 downsampling layer를 stage가 끝날때마다 stride가 2인 2x2 conv later를 이용하여 추가함. 다만 학습이 안정화되지 않아 normarlization layer를 추가하는 것으로 학습을 안정화시킴
-> accuracy : 81.5% -> 82.0% (+0.5%)

이러한 단계를 거쳐 개발된 ConvNeXt 모델은 Swin Transformer와 유사한 FLOPs, 파라미터 수, 처리량, 메모리 사용량을 가지면서도, Swin Transformer에 필요한 특별한 모듈 없이도 뛰어난 성능을 보입니다.


※ FLOP(FLoating-point OPerations)
: 컴퓨터가 수행하는 부동 소수점 연산 횟수를 의미합니다. 딥러닝 모델의 복잡도를 나타내는 지표로 널리 사용되며, 모델의 학습 및 추론에 필요한 계산량을 추정하는 데 활용됩니다. FLOP이 높을수록 모델이 더 복잡하고 많은 계산이 필요하다는 것을 의미합니다. \
쉽게 말해, FLOP은 모델이 얼마나 많은 계산을 해야 하는지를 나타내는 숫자라고 생각하면 됩니다. 예를 들어, 덧셈, 뺄셈, 곱셈, 나눗셈과 같은 연산을 몇 번 해야 하는지 세는 것이죠. 딥러닝 모델은 수많은 연산을 수행하므로 FLOP은 모델의 크기나 성능을 비교할 때 중요한 기준이 됩니다. 

![figure2](https://github.com/yyeongha/yyeongha.github.io/blob/main/assets/img/favicons/2024-05-15-img/figure2.png?raw=true)


## 2.1. Training Techniques
네트워크 아키텍처의 설계 외에도 훈련 절차는 궁극적인 성능에도 영향을 미칩니다. 비전 트랜스포머는 새로운 모듈 세트와 아키텍처 설계 결정을 가져왔을 뿐만 아니라 비전에 서로 다른 교육 기술(예: AdamW 옵티마이저)을 도입했습니다. 이것은 주로 최적화 전략 및 관련 하이퍼 매개변수 설정과 관련이 있습니다. 따라서 탐색의 첫 번째 단계는 비전 트랜스포머 교육 절차, 즉 ResNet50/200을 사용하여 기준선 모델을 교육하는 것입니다. \
최근 연구에서는 현대적인 교육 기술 세트가 간단한 ResNet-50 모델의 성능을 크게 향상시킬 수 있음을 보여줍니다. 본 연구에서는 DeiT  및 Swin Transformer에 가까운 교육 레시피를 사용합니다. ResNet의 경우 교육이 원래 90 에포크에서 300 에포크로 확장됩니다. AdamW 옵티마이저, Mixup, Cutmix, RandAugment, Random Erasing과 같은 데이터 증강 기술 및 Stochastic Depth 및 Label Smoothing를 포함한 정규화 체계를 사용합니다. 우리가 사용하는 전체 하이퍼 매개변수 세트는 부록 A.1에서 찾을 수 있습니다.

이 향상된 교육 레시피 자체만으로도 ResNet-50 모델의 성능이 76.1%에서 78.8%(+2.7%)로 증가하여 기존 ConvNet과 비전 트랜스포머 간의 성능 차이의 상당 부분이 교육 기술 때문일 수 있음을 의미합니다. "현대화" 프로세스 전체에서 동일한 하이퍼 매개변수와 함께 이 고정 교육 레시피를 사용합니다. ResNet-50 체제에 대해 보고된 각 정확도는 세 가지 서로 다른 무작위 시드로 훈련하여 얻은 평균입니다.

## 2.2. Macro Design
이제 Swin Transformers의 매크로 네트워크 설계를 분석합니다. Swin Transformers는 ConvNets에 따라 각 단계마다 다른 기능 맵 해상도를 갖는 다단계 설계를 사용합니다. 스테이지 컴퓨팅 비율과 "줄기 세포" 구조라는 두 가지 흥미로운 디자인 고려 사항이 있습니다.

### 단계별 계산 비율 변경. 
ResNet에서 단계 간 계산 분포의 원래 설계는 대체로 경험적이었습니다. 무거운 "res4" 단계는 감지기 헤드가 14×14 피쳐 평면에서 작동하는 객체 감지와 같은 다운스트림 작업과 호환되도록 의도되었습니다. \
반면 Swin-T는 동일한 원칙을 따랐지만 약간 다른 1:1:3:1의 단계 계산 비율을 사용했습니다. 더 큰 Swin Transformers의 경우 비율은 1:1:9:1입니다. \
설계에 따라 각 단계의 블록 수를 ResNet-50의 (3, 4, 6, 3)에서 (3, 3, 9, 3)으로 조정하여 FLOP를 Swin-T와 정렬합니다. 이렇게 하면 모델 정확도가 78.8%에서 79.4%로 향상됩니다. \
특히 연구원들은 계산 분포를 철저히 조사했으며 더 최적화된 설계가 존재할 가능성이 높습니다. 지금부터 이 단계 계산 비율을 사용하겠습니다.

### 줄기를 "Patchify"로 변경. 
일반적으로 줄기 세포 설계는 입력 이미지가 네트워크 시작 부분에서 어떻게 처리될지에 대한 것입니다. \
자연 이미지에 내재된 중복성으로 인해 일반적인 줄기 세포는 표준 ConvNet 및 비전 트랜스포머 모두에서 입력 이미지를 적절한 피처 맵 크기로 적극적으로 다운샘플링합니다. \
표준 ResNet의 줄기 세포에는 보폭 2의 7×7 컨볼루션 계층과 그 뒤에 최대 풀이 포함되어 있어 입력 이미지가 4배 다운샘플링됩니다. 비전 트랜스포머에서는 더 공격적인 "패치화" 전략이 줄기 세포로 사용되는데, 이는 큰 커널 크기(예: 커널 크기 = 14 또는 16) 및 겹치지 않는 컨볼루션에 해당합니다. \
Swin Transformer는 유사한 "패치화" 레이어를 사용하지만 아키텍처의 다단계 설계를 수용하기 위해 더 작은 패치 크기 4를 사용합니다. \
ResNet 스타일 줄기 세포를 4×4, 보폭 4 컨볼루션 레이어를 사용하여 구현된 패치 레이어로 교체합니다. \
정확도는 79.4%에서 79.5%로 변경되었습니다. 이는 ResNet의 줄기 세포를 ViT와 유사한 더 간단한 "패치화" 레이어로 대체하면 유사한 성능을 얻을 수 있음을 시사합니다. \
이제 네트워크에서 "패치 줄기"(4×4 비 중첩 컨볼루션)를 사용합니다.


## 2.3. ResNeXt-ify
이 부분에서는 ResNeXt라는 모델의 아이디어를 ResNet에 적용하는 과정을 설명합니다. ResNeXt는 ResNet보다 연산량 대비 정확도가 더 좋은 모델입니다. ResNeXt의 핵심은 그룹 컨볼루션(grouped convolution)을 사용하는 것인데, 컨볼루션 필터를 여러 그룹으로 나누어 연산하는 방식입니다. ResNeXt의 기본 원리는 "그룹을 더 많이 사용하고, 너비(채널 수)를 확장한다"는 것입니다. 그룹 컨볼루션을 사용하면 연산량이 줄어들기 때문에, 줄어든 정보량을 보상하기 위해 네트워크의 너비를 늘립니다. 본 논문에서는 그룹 컨볼루션의 특별한 형태인 깊이별 컨볼루션(depthwise convolution)을 사용합니다. 깊이별 컨볼루션은 그룹 수가 채널 수와 같은 경우이며, MobileNet과 Xception에서 널리 사용되었습니다. 깊이별 컨볼루션은 각 채널별로 독립적인 연산을 수행하므로 공간 정보만 혼합하고 채널 정보는 혼합하지 않습니다. 이는 Vision Transformer의 self-attention과 유사한 특징입니다. ResNet-50 모델에 깊이별 컨볼루션을 적용하면 네트워크의 연산량은 줄어들지만 정확도도 감소합니다. 따라서 ResNeXt 전략에 따라 네트워크의 너비를 Swin-T 모델과 동일한 96개 채널로 늘려줍니다. 이렇게 하면 FLOPs(연산량)는 5.3G로 증가하지만 정확도는 80.5%로 향상됩니다.

※ 그룹 컨볼루션(Grouped Convolution) \
: 컨볼루션 신경망(CNN)에서 사용되는 합성곱 연산의 한 종류입니다. 일반적인 합성곱 연산은 입력 채널 전체에 대해 하나의 필터를 적용하는 반면, 그룹 컨볼루션은 입력 채널을 여러 그룹으로 나누고 각 그룹에 대해 별도의 필터를 적용

※ 깊이별 컨볼루션(depthwise convolution) \
: 그룹 컨볼루션(grouped convolution)의 특별한 경우로, 각 그룹이 하나의 입력 채널만을 담당하는 합성곱 연산입니다. 다시 말해, 깊이별 컨볼루션은 각 채널에 대해 독립적인 필터를 적용하여 특징 맵을 생성하는 방식입니다.

※ Vision Transformer(ViT) \
: 자연어 처리 분야에서 널리 사용되는 Transformer 모델을 이미지 인식 작업에 적용한 모델입니다. 기존의 컨볼루션 신경망(CNN)과는 달리, ViT는 이미지를 작은 패치(patch)로 나누고 각 패치를 자연어 처리에서 단어처럼 취급하여 패치 간의 관계를 분석하고 이미지 전체를 이해합니다.

※ Self-attention \
: 트랜스포머 모델의 핵심 구성 요소로, 입력 데이터의 각 요소가 다른 모든 요소와의 관계를 계산하여 중요도를 파악하는 메커니즘 \
쉽게 설명하면, self-attention은 문장 속의 각 단어가 다른 단어들과 얼마나 관련이 있는지를 파악하는 것과 비슷합니다. 예를 들어 "나는 오늘 맛있는 사과를 먹었다"라는 문장에서 "사과"라는 단어는 "먹었다"라는 단어와 높은 관련성을 가지지만, "나는"이나 "오늘"과는 관련성이 낮습니다. Self-attention은 이러한 관련성을 수치화하여 각 단어의 중요도를 나타내는 가중치를 부여합니다.


## 2.4. Inverted Bottleneck
모든 Transformer 블록의 중요한 설계 중 하나는 inverted bottleneck을 생성한다는 것입니다. 즉, MLP 블록의 hidden dimension이 입력 dimension보다 4배 더 넓습니다(그림 4 참조). 흥미롭게도 이 Transformer 디자인은 ConvNet에서 확장 비율이 4인 inverted bottleneck 디자인과 연결됩니다. 이 아이디어는 MobileNetV2에 의해 대중화되었으며 이후 여러 고급 ConvNet 아키텍처에서 인기를 얻었습니다. \
![figure4](https://github.com/yyeongha/yyeongha.github.io/blob/main/assets/img/favicons/2024-05-15-img/figure4.png?raw=true) 

여기에서 inverted bottleneck 디자인을 살펴봅니다. 그림 3 (a)에서 (b)는 구성을 보여줍니다. 깊이별 컨볼루션 레이어의 FLOP가 증가했음에도 불구하고 이 변경으로 인해 다운샘플링 residual 블록의 shortcut 1x1 conv 레이어에서 FLOP가 크게 감소하여 전체 네트워크 FLOP가 4.6G로 감소합니다. 흥미롭게도 이는 성능이 약간 향상됩니다(80.5%에서 80.6%). ResNet-200/Swin-B 체제에서 이 단계는 FLOP 감소와 함께 더 많은 이득(81.9%에서 82.6%)을 가져옵니다. \
![figure3](https://github.com/yyeongha/yyeongha.github.io/blob/main/assets/img/favicons/2024-05-15-img/figure3.png?raw=true)

이제 inverted bottleneck을 사용합니다.

※ MLP(Multi-Layer Perceptron) \
: 다층 퍼셉트론이라고도 불리며, 가장 기본적인 형태의 인공 신경망입니다. 입력층, 은닉층(hidden layer), 출력층으로 구성되어 있으며, 각 층은 여러 개의 뉴런(노드)으로 이루어져 있습니다. 각 뉴런은 입력 값을 받아 가중치를 곱하고 활성화 함수를 거쳐 출력 값을 생성합니다. 이러한 과정을 통해 MLP는 복잡한 패턴을 학습하고 예측할 수 있습니다


## 2.5. Large Kernel Sizes
이 탐색 부분에서는 큰 컨볼루션 커널의 동작에 초점을 맞춥니다. 비전 트랜스포머의 가장 두드러진 측면 중 하나는 각 레이어가 글로벌 수용 필드를 가질 수 있도록 하는 비-로컬 자기 주의입니다. 과거에는 ConvNet 에 큰 커널 크기가 사용되었지만, 최신 GPU에서 효율적인 하드웨어 구현을 갖는 작은 커널 크기(3x3) 컨볼루션 레이어를 쌓는 것이 일반적인 표준입니다(VGGNet에서 대중화됨). Swin Transformers는 로컬 윈도우를 다시 self-attention 블록에 도입했지만 윈도우 크기는 최소 7x7로 ResNe(X)t 커널 크기 3x3보다 훨씬 큽니다. 여기서 우리는 ConvNet을 위한 큰 커널 크기 컨볼루션의 사용을 재검토합니다.

### 깊이별 합성곱 레이어 위로 이동: 
큰 커널을 탐색하기 위한 전제 조건 중 하나는 깊이별 컨볼루션 레이어의 위치를 위로 이동하는 것입니다(그림 3 (b)에서 (c)). 이는 Transformer에서도 분명한 설계 결정입니다. MSA 블록은 MLP 레이어 이전에 배치됩니다. inverted bottleneck 블록이 있으므로 복잡/비효율적인 모듈(MSA, large-kernel conv)은 더 적은 채널을 갖는 반면 효율적이고 조밀한 1x1 레이어는 무거운 작업을 수행하기 때문에 이는 자연스러운 설계 선택입니다. 이 중간 단계는 FLOP를 4.1G로 줄여 일시적으로 성능이 79.9%로 저하됩니다. \
![figure3](https://github.com/yyeongha/yyeongha.github.io/blob/main/assets/img/favicons/2024-05-15-img/figure3.png?raw=true)

### 커널 크기 늘리기: 
이러한 모든 준비를 통해 더 큰 커널 크기의 컨볼루션을 채택하면 상당한 이점이 있습니다. 3, 5, 7, 9 및 11을 포함하여 여러 커널 크기를 실험했습니다. 네트워크의 성능은 79.9%(3x3)에서 80.6%(7x7)로 증가하는 반면 네트워크의 FLOP는 거의 동일하게 유지됩니다. 또한 더 큰 커널 크기의 이점은 7x7에서 포화점에 도달한다는 것을 관찰했습니다. 대용량 모델에서도 이러한 동작을 확인했습니다. ResNet-200 체제 모델은 커널 크기를 7x7 이상으로 늘려도 더 이상 이득을 얻지 못합니다.

각 블록에서 7x7 깊이별 컨볼루션을 사용합니다.

이 시점에서 매크로 규모의 네트워크 아키텍처 검토를 마쳤습니다. 흥미롭게도 비전 트랜스포머에서 채택된 설계 선택의 상당 부분은 ConvNet 인스턴스화에 매핑될 수 있습니다.


## 2.6. Micro Design
이 섹션에서는 마이크로 수준에서 몇 가지 다른 아키텍처 차이점을 조사합니다. 여기에서 대부분의 탐색은 활성화 함수 및 정규화 계층의 특정 선택에 초점을 맞춰 레이어 수준에서 수행됩니다.

### ReLU를 GELU로 교체
: NLP 아키텍처와 비전 아키텍처 간의 한 가지 불일치는 사용할 활성화 함수의 세부 사항입니다. 시간이 지남에 따라 수많은 활성화 함수가 개발되었지만 Rectified Linear Unit(ReLU) [49]은 단순성과 효율성으로 인해 ConvNet에서 여전히 광범위하게 사용됩니다. ReLU는 원래 Transformer 논문 [77]에서 활성화 함수로도 사용됩니다. ReLU의 더 부드러운 변형으로 생각할 수 있는 GELU(Gaussian Error Linear Unit) [32]는 Google BERT [18] 및 OpenAI의 GPT-2 [52]를 포함하여 가장 진보된 Transformer에서 사용됩니다. 그리고 가장 최근에는 ViT에도 사용됩니다. ConvNet에서도 ReLU를 GELU로 대체할 수 있지만 정확도는 변경되지 않습니다(80.6%).

### 더 적은 활성화 함수: 
Transformer와 ResNet 블록 간의 사소한 차이점 중 하나는 Transformer에 활성화 함수가 더 적다는 것입니다. 키/쿼리/값 선형 임베딩 레이어, 투영 레이어 및 MLP 블록의 두 개의 선형 레이어가 있는 Transformer 블록을 고려하십시오. MLP 블록에는 활성화 함수가 하나만 있습니다. 이에 비해 1x1 conv를 포함하여 각 컨볼루션 레이어에 활성화 함수를 추가하는 것이 일반적입니다. 여기서 우리는 동일한 전략을 고수할 때 성능이 어떻게 변하는지 조사합니다. 그림 4에 나와 있는 것처럼 Transformer 블록 스타일을 복제하여 두 개의 1x1 레이어 사이에 하나를 제외한 residual 블록에서 모든 GELU 레이어를 제거합니다. 이 과정은 결과를 0.7%에서 81.3%로 향상시켜 Swin-T의 성능과 거의 일치시킵니다. 이제 각 블록에 단일 GELU 활성화를 사용합니다.

### 더 적은 정규화 레이어: 
Transformer 블록에는 일반적으로 정규화 레이어도 더 적습니다. 여기에서 conv 1x1 레이어 앞에 하나의 BN 레이어만 남겨두고 두 개의 BatchNorm(BN) 레이어를 제거합니다. 이렇게 하면 이미 Swin-T의 결과를 능가하는 81.4%로 성능이 더욱 향상됩니다. 블록 시작 부분에 하나의 추가 BN 레이어를 추가해도 성능이 향상되지 않는다는 것을 경험적으로 알 수 있으므로 Transformer보다 블록당 정규화 레이어가 훨씬 더 적습니다.

### BN을 LN으로 대체: 
배치 정규화(BN) [38]은 수렴을 개선하고 과적합을 줄이기 때문에 ConvNet의 필수 구성 요소입니다. 그러나 BN에는 모델 성능에 악영향을 줄 수 있는 많은 복잡성도 있습니다 [84]. 대체 정규화 [60, 75, 83] 기술을 개발하려는 수많은 시도가 있었지만 BN은 대부분의 비전 작업에서 선호되는 옵션으로 남아 있습니다. 반면에 더 간단한 Layer Normalization [5] (LN)은 Transformer에서 사용되어 다양한 응용 시나리오에서 좋은 성능을 얻었습니다. 원래 ResNet에서 LN을 BN으로 직접 대체하면 성능이 저하됩니다 [83]. 네트워크 아키텍처 및 교육 기술의 모든 수정 사항과 함께 여기에서 BN 대신 LN을 사용하는 영향을 다시 살펴봅니다. ConvNet 모델은 LN으로 훈련하는 데 어려움이 없습니다. 사실 성능이 약간 더 좋으며 81.5%의 정확도를 얻습니다. 지금부터 각 residual 블록에서 정규화 선택으로 하나의 LayerNorm을 사용합니다.

### 별도의 다운샘플링 레이어: 
ResNet에서 공간 다운샘플링은 각 단계 시작 부분에 있는 residual 블록에서 보폭 2의 3x3 conv(및 shortcut 연결에서 보폭 2의 1x1 conv)를 사용하여 수행됩니다. Swin Transformers에서는 별도의 다운샘플링 레이어가 스테이지 사이에 추가됩니다. 공간 다운샘플링을 위해 보폭 2의 2x2 conv 레이어를 사용하는 유사한 전략을 탐색합니다. 이 수정은 놀랍게도 훈련이 분기됩니다. 추가 조사에 따르면 공간 해상도가 변경될 때마다 정규화 레이어를 추가하면 교육 안정화에 도움이 될 수 있습니다. 여기에는 Swin Transformers에서도 사용되는 여러 LN 레이어가 포함됩니다. 각 다운샘플링 레이어 앞에 하나, 줄기 뒤에 하나, 마지막 글로벌 평균 풀링 후에 하나입니다. 정확도를 82.0%로 높여 Swin-T의 81.3%를 크게 웃돌 수 있습니다. 별도의 다운샘플링 레이어를 사용합니다. 이것이 ConvNeXt라고 불리는 최종 모델입니다.

ResNet, Swin 및 ConvNeXt 블록 구조 비교는 그림 4에서 찾을 수 있습니다. ResNet-50, Swin-T 및 ConvNeXt-T의 자세한 아키텍처 사양 비교는 표 9에서 찾을 수 있습니다.

### 마무리 발언: 
첫 번째 "플레이 스루"를 마치고 순수 ConvNet인 ConvNeXt를 발견했습니다. 이 컴퓨팅 체제에서 ImageNet-1K 분류를 위해 Swin Transformer보다 성능이 뛰어날 수 있습니다. 지금까지 논의된 모든 설계 선택은 비전 트랜스포머에서 채택되었습니다. 또한 이러한 디자인은 ConvNet 문헌에서도 새로운 것이 아닙니다. 지난 10년 동안 모두 별도로 연구되었지만 집합적으로 연구되지는 않았습니다. ConvNeXt 모델은 Swin Transformer와 거의 동일한 FLOP, #params., 처리량 및 메모리 사용량을 갖지만 이동된 윈도우 주의 또는 상대 위치 편향과 같은 특수 모듈이 필요하지 않습니다.

이러한 발견은 고무적이지만 아직 완전히 설득력이 있는 것은 아닙니다. 지금까지 우리의 탐구는 소규모로 제한되었지만 비전 트랜스포머의 확장 행동은 진정으로 그들을 구별하는 것입니다. 또한 ConvNet이 객체 감지 및 의미 분할과 같은 다운스트림 작업에서 Swin Transformers와 경쟁할 수 있는지 여부는 컴퓨터 비전 실무자들의 주요 관심사입니다. 다음 섹션에서는 ConvNeXt 모델의 데이터 및 모델 크기를 확장하고 다양한 시각적 인식 작업 세트에서 평가합니다.


-----------------------------------
## experiments
![experiment]()
* ViT, Swin과 같이 다양한 사이즈의 모델들을 구성함
* 가운데는 output의 채널 수, 마지막은 block의 수
블록의 수는 1:1:3:1의 비율을 가지고 있다.

![]()
* swin과 비교할 경우 같은 조건하에 convnext가 더 좋은 성능을 보이며, * indective bias에 더 큰 데이터셋에서 우세를 보이던 ViT를 웃도는 성능을 보임
* classification을 제외한 다른 task에서도 우수한 성능을 보이며 연산량인 FLOP도 적은 것을 볼 수 있다.

## conclutsion
* convnext는 pure convnet으로만 이루어져 잇으며, 이것만으로도 imagenet-1k와 같은 거대한 데이터셋에 대해서도 swin의 성능을 보일 수 있는 것으로 보임
* convnext는 swin에서 사용한 swifted window attention, relative position bias와 같은 specialized module을 따로 요구하지 않음. 그럼에도 더 낮은 FLOPs을 보임























# 3. Empirical Evaluations on ImageNet
우리는 Swin-T/S/B/L [45]과 유사한 복잡성을 갖도록 서로 다른 ConvNeXt 변형 ConvNeXt-T/S/B/L을 구성합니다. ConvNeXt-T/B는 각각 ResNet-50/200 체제에 대한 "현대화" 절차의 최종 결과물입니다. 또한 ConvNeXt의 확장성을 추가로 테스트하기 위해 더 큰 ConvNeXt-XL을 구축합니다. 변형은 채널 수 C와 각 단계의 블록 수 B만 다릅니다. ResNet 및 Swin Transformers를 모두 따르면 각 새 단계에서 채널 수가 두 배가 됩니다. 아래 구성을 요약합니다.

ConvNeXt-T: C = (96, 192, 384, 768), B = (3, 3, 9, 3)
ConvNeXt-S: C = (96, 192, 384, 768), B = (3, 3, 27, 3)
ConvNeXt-B: C = (128, 256, 512, 1024), B = (3, 3, 27, 3)
ConvNeXt-L: C = (192, 384, 768, 1536), B = (3, 3, 27, 3)
ConvNeXt-XL: C = (256, 512, 1024, 2048), B = (3, 3, 27, 3) 3.1. 설정
ImageNet-1K 데이터 세트는 120만 개의 교육 이미지가 있는 1000개의 객체 클래스로 구성됩니다. 검증 세트에 대한 ImageNet-1K 상위 1개 정확도를 보고합니다. 또한 ImageNet-22K에 대한 사전 교육을 수행합니다. ImageNet-22K는 교육을 위해 약 14M 이미지가 있는 21841개 클래스(1000개 ImageNet-1K 클래스의 상위 집합)의 더 큰 데이터 세트이며 그런 다음 평가를 위해 ImageNet-1K에서 사전 훈련된 모델을 미세 조정합니다. 아래 교육 설정을 요약합니다. 자세한 내용은 부록 A에서 확인할 수 있습니다.

## 3.1. Settings
### ImageNet-1K 교육: 
4e-3의 학습률로 AdamW [46]를 사용하여 300 에포크 동안 ConvNeXt를 훈련합니다. 20 에포크 선형 준비(warmup)가 있으며 이후 코사인 감쇠 일정이 있습니다. 배치 크기 4096 및 가중치 감쇠 0.05를 사용합니다. 데이터 증강을 위해 Mixup [90], Cutmix [89], RandAugment [14] 및 Random Erasing [91]을 포함한 일반적인 체계를 채택합니다. Stochastic Depth [37] 및 Label Smoothing [69]을 사용하여 네트워크를 정규화합니다. 초기 값 1e-6의 레이어 스케일 [74]가 적용됩니다. EMA(Exponential Moving Average) [51]를 사용하면 더 큰 모델의 과적합을 완화할 수 있습니다.

### ImageNet-22K 사전 훈련: 
5 에포크의 준비(warmup)와 함께 ImageNet-22K에서 90 에포크 동안 ConvNeXt를 사전 훈련합니다. EMA는 사용하지 않습니다. 다른 설정은 ImageNet-1K를 따릅니다.

### ImageNet-1K에 대한 미세 조정: 
ImageNet-1K에서 ImageNet-22K 사전 훈련된 모델을 30 에포크 동안 미세 조정합니다. AdamW, 학습률 5e-5, 코사인 학습률 일정, 레이어별 학습률 감소 [6, 12], 준비(warmup) 없음, 배치 크기 512 및 가중치 감소 1e-8을 사용합니다. 기본 사전 훈련, 미세 조정 및 테스트 해상도는 224^2입니다. 또한 ImageNet-22K 및 ImageNet-1K 사전 훈련된 모델 모두에 대해 384^2의 더 큰 해상도로 미세 조정합니다.

ViT/Swin Transformers와 비교하여 ConvNeXt는 네트워크가 완전히 컨볼루션이고 입력 패치 크기를 조정하거나 절대/상대 위치 편향을 보간할 필요가 없기 때문에 다른 해상도에서 미세 조정하기가 더 간단합니다.


## 3.2. Results
### ImageNet-1K: 
표 1(위쪽)에는 두 가지 최신 Transformer 변형인 DeiT [73] 및 Swin Transformers [45]와 아키텍처 검색의 두 가지 ConvNet(RegNets [54], EfficientNets [71] 및 EfficientNetsV2 [72])과의 결과 비교가 나와 있습니다. ConvNeXt는 정확도-계산 트레이드 오프 및 추론 처리량 측면에서 두 가지 강력한 ConvNet 기준선(RegNet[54] 및 EfficientNet[71])과 경쟁합니다. ConvNeXt는 또한 전체 모델에서 유사한 복잡성을 가진 Swin Transformer보다 성능이 뛰어나며 때로는 상당한 차이(예: ConvNeXt-T의 경우 0.8%)를 보입니다. 이동된 윈도우 또는 상대 위치 편향과 같은 특수 모듈이 없는 ConvNeXt는 Swin Transformers에 비해 처리량도 향상되었습니다.
결과의 하이라이트는 384^2에서 ConvNeXt-B입니다. Swin-B보다 0.6%(85.1% 대 84.5%) 더 나은 성능을 보이지만 추론 처리량은 12.5% 더 높습니다(95.7 대 85.1 이미지/초). ConvNeXt-B가 Swin-B보다 FLOPs/처리량 이점이 해상도가 224^2에서 384^2로 증가할 때 더 커진다는 점에 유의하십시오. 또한 ConvNeXt-L로 더 확장하면 85.5%의 향상된 결과를 관찰합니다.

### ImageNet-22K: 
표 1(아래쪽)에 ImageNet-22K 사전 훈련에서 미세 조정된 모델의 결과를 제시합니다. 이러한 실험은 비전 Transformer가 더 적은 inductive bias를 가지므로 대규모로 사전 훈련될 때 ConvNet보다 더 나은 성능을 발휘할 수 있다는 널리 알려진 견해이기 때문에 중요합니다. 결과는 적절하게 설계된 ConvNet이 대규모 데이터 세트로 사전 훈련될 때 비전 Transformer보다 성능이 떨어지지 않는다는 것을 보여줍니다. ConvNeXt는 여전히 유사한 크기의 Swin Transformer만큼 또는 더 나은 성능을 발휘하며 처리량이 약간 더 높습니다. 또한 ConvNeXt-XL 모델은 87.8%의 정확도를 달성하여 384^2에서 ConvNeXt-L보다 상당한 개선을 보여 ConvNeXt가 확장 가능한 아키텍처임을 보여줍니다.
ImageNet-1K에서 고급 모듈(예: Squeeze-and-Excitation [35])과 점진적인 교육 절차를 갖춘 검색 아키텍처인 EfficientNetV2-L은 최고의 성능을 달성합니다. 그러나 ImageNet-22K 사전 교육을 통해 ConvNeXt는 EfficientNetV2보다 성능이 뛰어나 대규모 교육의 중요성을 더욱 보여줍니다. 부록 B에서는 ConvNeXt에 대한 견고성 및 도메인 외 일반화 결과에 대해 설명합니다.


## 3.3. Isotropic ConvNeXt vs. ViT

















---