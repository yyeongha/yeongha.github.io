---
title: 논문리뷰_A ConvNet for the 2020s
categories: [논문리뷰] 
date: 2024-05-15
last_modified_at: 2024-05-15
---

논문 출처 : https://arxiv.org/abs/2201.03545v2


# 1. Introduction
2010년대 컴퓨터 비전 분야는 이미지 인식에 탁월한 합성곱 신경망(ConvNets)의 발전으로 급성장했습니다. 2020년, 자연어 처리 분야의 트랜스포머 모델이 ViT(Vision Transformer)라는 이름으로 컴퓨터 비전에 도입되면서 새로운 변화를 맞이했습니다. ViT는 이미지 분류에는 뛰어났지만 객체 감지나 이미지 분할과 같은 작업에는 한계를 보였습니다. 이후 Swin Transformer와 같은 계층적 트랜스포머 모델이 등장하여 ConvNets의 장점을 일부 차용하며 다양한 컴퓨터 비전 작업에서 좋은 성능을 보였습니다. 본 논문에서는 ConvNets를 개선하여 트랜스포머 모델만큼의 성능을 내는 ConvNeXt라는 새로운 모델을 제안합니다. ConvNeXt는 다양한 벤치마크에서 Swin Transformer와 비슷하거나 더 나은 성능을 보였고, 기존 ConvNets의 단순함과 효율성도 유지했습니다. 본 연구는 ConvNets가 여전히 컴퓨터 비전 분야에서 중요한 역할을 할 수 있음을 보여주며, 컴퓨터 비전 모델 설계에 새로운 시각을 제시합니다.


# 2. Modernizing a ConvNet: a Roadmap
이 섹션에서는 ResNet에서 Transformer와 유사한 ConvNet으로 이동하는 궤적을 제공합니다. FLOP 측면에서 두 가지 모델 크기를 고려합니다. 하나는 FLOP가 약 4.5×10^9인 ResNet-50/Swin-T 체제이고 다른 하나는 FLOP가 약 15.0×10^9인 ResNet-200/Swin-B 체제입니다. 간단하게 ResNet-50/Swin-T 복잡도 모델의 결과를 보여드리겠습니다. 더 높은 용량 모델에 대한 결론은 일치하며 결과는 부록 C에서 찾을 수 있습니다.

※ FLOP(FLoating-point OPerations)
:  컴퓨터가 수행하는 부동 소수점 연산 횟수를 의미합니다. 딥러닝 모델의 복잡도를 나타내는 지표로 널리 사용되며, 모델의 학습 및 추론에 필요한 계산량을 추정하는 데 활용됩니다. FLOP이 높을수록 모델이 더 복잡하고 많은 계산이 필요하다는 것을 의미합니다. \
쉽게 말해, FLOP은 모델이 얼마나 많은 계산을 해야 하는지를 나타내는 숫자라고 생각하면 됩니다. 예를 들어, 덧셈, 뺄셈, 곱셈, 나눗셈과 같은 연산을 몇 번 해야 하는지 세는 것이죠. 딥러닝 모델은 수많은 연산을 수행하므로 FLOP은 모델의 크기나 성능을 비교할 때 중요한 기준이 됩니다. 

높은 수준에서 우리의 탐색은 표준 ConvNet으로서 네트워크의 단순성을 유지하면서 Swin Transformer의 다양한 수준의 디자인을 조사하고 따르도록 지시됩니다. 탐색 로드맵은 다음과 같습니다. 시작점은 ResNet-50 모델입니다. 먼저 비전 트랜스포머를 교육하는 데 사용되는 유사한 교육 기술로 교육하고 원래 ResNet-50에 비해 훨씬 향상된 결과를 얻습니다. 이것이 우리의 기준선이 될 것입니다. 그런 다음 1) Macro Design, 2) ResNeXt, 3) Inverted Bottleneck, 4) Large Kernel 5) 다양한 레이어별 Micro Design으로 요약한 일련의 디자인 결정을 연구합니다. 그림 2에서는 "네트워크 현대화"의 각 단계를 통해 절차와 결과를 보여줍니다. 네트워크 복잡성은 최종 성능과 밀접하게 관련되어 있으므로 탐색 과정에서 FLOP는 대략적으로 제어되지만 중간 단계에서는 FLOP가 참조 모델보다 높거나 낮을 수 있습니다. 모든 모델은 ImageNet-1K에서 훈련 및 평가됩니다. 

![figure2](https://github.com/yyeongha/yyeongha.github.io/blob/main/assets/img/favicons/2024-05-15-img/figure2.png?raw=true)
어텐션 기반 모듈을 도입하지 않고 표준 ConvNet(ResNet)을 계층적 비전 Transformer(Swin) 설계 방향으로 현대화합니다. 전경 막대는 ResNet-50/Swin-T FLOP 체제의 모델 정확도입니다. ResNet-200/Swin-B 체제의 결과는 회색 막대로 표시됩니다. 빗금 막대는 수정이 채택되지 않았음을 의미합니다. 두 체제에 대한 자세한 결과는 부록에 있습니다. 많은 Transformer 아키텍처 선택이 ConvNet에 통합될 수 있으며 성능이 점점 더 향상됩니다. 결국 ConvNeXt라는 순수 ConvNet 모델은 Swin Transformer보다 성능이 뛰어날 수 있습니다. 

### 2.1. Training Techniques
네트워크 아키텍처의 설계 외에도 훈련 절차는 궁극적인 성능에도 영향을 미칩니다. 비전 트랜스포머는 새로운 모듈 세트와 아키텍처 설계 결정을 가져왔을 뿐만 아니라 비전에 서로 다른 교육 기술(예: AdamW 옵티마이저)을 도입했습니다. 이것은 주로 최적화 전략 및 관련 하이퍼 매개변수 설정과 관련이 있습니다. 따라서 탐색의 첫 번째 단계는 비전 트랜스포머 교육 절차, 즉 ResNet50/200을 사용하여 기준선 모델을 교육하는 것입니다. 최근 연구에서는 현대적인 교육 기술 세트가 간단한 ResNet-50 모델의 성능을 크게 향상시킬 수 있음을 보여줍니다. 본 연구에서는 DeiT  및 Swin Transformer에 가까운 교육 레시피를 사용합니다. ResNet의 경우 교육이 원래 90 에포크에서 300 에포크로 확장됩니다. AdamW 옵티마이저, Mixup, Cutmix, RandAugment, Random Erasing과 같은 데이터 증강 기술 및 Stochastic Depth 및 Label Smoothing를 포함한 정규화 체계를 사용합니다. 우리가 사용하는 전체 하이퍼 매개변수 세트는 부록 A.1에서 찾을 수 있습니다.

이 향상된 교육 레시피 자체만으로도 ResNet-50 모델의 성능이 76.1%에서 78.8%(+2.7%)로 증가하여 기존 ConvNet과 비전 트랜스포머 간의 성능 차이의 상당 부분이 교육 기술 때문일 수 있음을 의미합니다. "현대화" 프로세스 전체에서 동일한 하이퍼 매개변수와 함께 이 고정 교육 레시피를 사용합니다. ResNet-50 체제에 대해 보고된 각 정확도는 세 가지 서로 다른 무작위 시드로 훈련하여 얻은 평균입니다.

### 2.2. Macro Design
이제 Swin Transformers의 매크로 네트워크 설계를 분석합니다. Swin Transformers는 ConvNets에 따라 각 단계마다 다른 기능 맵 해상도를 갖는 다단계 설계를 사용합니다. 스테이지 컴퓨팅 비율과 "줄기 세포" 구조라는 두 가지 흥미로운 디자인 고려 사항이 있습니다.

#### 단계별 계산 비율 변경. 
ResNet에서 단계 간 계산 분포의 원래 설계는 대체로 경험적이었습니다. 무거운 "res4" 단계는 감지기 헤드가 14×14 피쳐 평면에서 작동하는 객체 감지와 같은 다운스트림 작업과 호환되도록 의도되었습니다. 반면 Swin-T는 동일한 원칙을 따랐지만 약간 다른 1:1:3:1의 단계 계산 비율을 사용했습니다. 더 큰 Swin Transformers의 경우 비율은 1:1:9:1입니다. 설계에 따라 각 단계의 블록 수를 ResNet-50의 (3, 4, 6, 3)에서 (3, 3, 9, 3)으로 조정하여 FLOP를 Swin-T와 정렬합니다. 이렇게 하면 모델 정확도가 78.8%에서 79.4%로 향상됩니다. 특히 연구원들은 계산 분포 [53, 54]를 철저히 조사했으며 더 최적화된 설계가 존재할 가능성이 높습니다. 지금부터 이 단계 계산 비율을 사용하겠습니다.

#### 줄기를 "Patchify"로 변경. 
일반적으로 줄기 세포 설계는 입력 이미지가 네트워크 시작 부분에서 어떻게 처리될지에 대한 것입니다. 자연 이미지에 내재된 중복성으로 인해 일반적인 줄기 세포는 표준 ConvNet 및 비전 트랜스포머 모두에서 입력 이미지를 적절한 피처 맵 크기로 적극적으로 다운샘플링합니다. 표준 ResNet의 줄기 세포에는 보폭 2의 7×7 컨볼루션 계층과 그 뒤에 최대 풀이 포함되어 있어 입력 이미지가 4× 다운샘플링됩니다. 비전 트랜스포머에서는 더 공격적인 "패치화" 전략이 줄기 세포로 사용되는데, 이는 큰 커널 크기(예: 커널 크기 = 14 또는 16) 및 겹치지 않는 컨볼루션에 해당합니다. Swin Transformer는 유사한 "패치화" 레이어를 사용하지만 아키텍처의 다단계 설계를 수용하기 위해 더 작은 패치 크기 4를 사용합니다. ResNet 스타일 줄기 세포를 4×4, 보폭 4 컨볼루션 레이어를 사용하여 구현된 패치 레이어로 교체합니다. 정확도는 79.4%에서 79.5%로 변경되었습니다. 이는 ResNet의 줄기 세포를 ViT와 유사한 더 간단한 "패치화" 레이어로 대체하면 유사한 성능을 얻을 수 있음을 시사합니다. 이제 네트워크에서 "패치 줄기"(4×4 비 중첩 컨볼루션)를 사용합니다.


## 2.3. ResNeXt-ify
이 부분에서는 ResNeXt라는 모델의 아이디어를 ResNet에 적용하는 과정을 설명합니다. ResNeXt는 ResNet보다 연산량 대비 정확도가 더 좋은 모델입니다. ResNeXt의 핵심은 그룹 컨볼루션(grouped convolution)을 사용하는 것인데, 컨볼루션 필터를 여러 그룹으로 나누어 연산하는 방식입니다. ResNeXt의 기본 원리는 "그룹을 더 많이 사용하고, 너비(채널 수)를 확장한다"는 것입니다. 그룹 컨볼루션을 사용하면 연산량이 줄어들기 때문에, 줄어든 정보량을 보상하기 위해 네트워크의 너비를 늘립니다. 본 논문에서는 그룹 컨볼루션의 특별한 형태인 깊이별 컨볼루션(depthwise convolution)을 사용합니다. 깊이별 컨볼루션은 그룹 수가 채널 수와 같은 경우이며, MobileNet과 Xception에서 널리 사용되었습니다. 깊이별 컨볼루션은 각 채널별로 독립적인 연산을 수행하므로 공간 정보만 혼합하고 채널 정보는 혼합하지 않습니다. 이는 Vision Transformer의 self-attention과 유사한 특징입니다. ResNet-50 모델에 깊이별 컨볼루션을 적용하면 네트워크의 연산량은 줄어들지만 정확도도 감소합니다. 따라서 ResNeXt 전략에 따라 네트워크의 너비를 Swin-T 모델과 동일한 96개 채널로 늘려줍니다. 이렇게 하면 FLOPs(연산량)는 5.3G로 증가하지만 정확도는 80.5%로 향상됩니다.

※ 그룹 컨볼루션(Grouped Convolution) \
: 컨볼루션 신경망(CNN)에서 사용되는 합성곱 연산의 한 종류입니다. 일반적인 합성곱 연산은 입력 채널 전체에 대해 하나의 필터를 적용하는 반면, 그룹 컨볼루션은 입력 채널을 여러 그룹으로 나누고 각 그룹에 대해 별도의 필터를 적용

※ 깊이별 컨볼루션(depthwise convolution) \
: 그룹 컨볼루션(grouped convolution)의 특별한 경우로, 각 그룹이 하나의 입력 채널만을 담당하는 합성곱 연산입니다. 다시 말해, 깊이별 컨볼루션은 각 채널에 대해 독립적인 필터를 적용하여 특징 맵을 생성하는 방식입니다.

※ Vision Transformer(ViT) \
: 자연어 처리 분야에서 널리 사용되는 Transformer 모델을 이미지 인식 작업에 적용한 모델입니다. 기존의 컨볼루션 신경망(CNN)과는 달리, ViT는 이미지를 작은 패치(patch)로 나누고 각 패치를 자연어 처리에서 단어처럼 취급하여 패치 간의 관계를 분석하고 이미지 전체를 이해합니다.

※ Self-attention \
: 트랜스포머 모델의 핵심 구성 요소로, 입력 데이터의 각 요소가 다른 모든 요소와의 관계를 계산하여 중요도를 파악하는 메커니즘 \
쉽게 설명하면, self-attention은 문장 속의 각 단어가 다른 단어들과 얼마나 관련이 있는지를 파악하는 것과 비슷합니다. 예를 들어 "나는 오늘 맛있는 사과를 먹었다"라는 문장에서 "사과"라는 단어는 "먹었다"라는 단어와 높은 관련성을 가지지만, "나는"이나 "오늘"과는 관련성이 낮습니다. Self-attention은 이러한 관련성을 수치화하여 각 단어의 중요도를 나타내는 가중치를 부여합니다.


## 2.4. Inverted Bottleneck
모든 Transformer 블록의 중요한 설계 중 하나는 inverted bottleneck을 생성한다는 것입니다. 즉, MLP 블록의 hidden dimension이 입력 dimension보다 4배 더 넓습니다(그림 4 참조). 흥미롭게도 이 Transformer 디자인은 ConvNet에서 확장 비율이 4인 inverted bottleneck 디자인과 연결됩니다. 이 아이디어는 MobileNetV2에 의해 대중화되었으며 이후 여러 고급 ConvNet 아키텍처에서 인기를 얻었습니다. \
![figure4](https://github.com/yyeongha/yyeongha.github.io/blob/main/assets/img/favicons/2024-05-15-img/figure4.png?raw=true) 

여기에서 inverted bottleneck 디자인을 살펴봅니다. 그림 3 (a)에서 (b)는 구성을 보여줍니다. 깊이별 컨볼루션 레이어의 FLOP가 증가했음에도 불구하고 이 변경으로 인해 다운샘플링 residual 블록의 shortcut 1x1 conv 레이어에서 FLOP가 크게 감소하여 전체 네트워크 FLOP가 4.6G로 감소합니다. 흥미롭게도 이는 성능이 약간 향상됩니다(80.5%에서 80.6%). ResNet-200/Swin-B 체제에서 이 단계는 FLOP 감소와 함께 더 많은 이득(81.9%에서 82.6%)을 가져옵니다. \
![figure3](https://github.com/yyeongha/yyeongha.github.io/blob/main/assets/img/favicons/2024-05-15-img/figure3.png?raw=true)

이제 inverted bottleneck을 사용합니다.

※ MLP(Multi-Layer Perceptron) \
: 다층 퍼셉트론이라고도 불리며, 가장 기본적인 형태의 인공 신경망입니다. 입력층, 은닉층(hidden layer), 출력층으로 구성되어 있으며, 각 층은 여러 개의 뉴런(노드)으로 이루어져 있습니다. 각 뉴런은 입력 값을 받아 가중치를 곱하고 활성화 함수를 거쳐 출력 값을 생성합니다. 이러한 과정을 통해 MLP는 복잡한 패턴을 학습하고 예측할 수 있습니다


## 2.5. Large Kernel Sizes
이 탐색 부분에서는 큰 컨볼루션 커널의 동작에 초점을 맞춥니다. 비전 트랜스포머의 가장 두드러진 측면 중 하나는 각 레이어가 글로벌 수용 필드를 가질 수 있도록 하는 비-로컬 자기 주의입니다. 과거에는 ConvNet 에 큰 커널 크기가 사용되었지만, 최신 GPU에서 효율적인 하드웨어 구현을 갖는 작은 커널 크기(3x3) 컨볼루션 레이어를 쌓는 것이 일반적인 표준입니다(VGGNet에서 대중화됨). Swin Transformers는 로컬 윈도우를 다시 self-attention 블록에 도입했지만 윈도우 크기는 최소 7x7로 ResNe(X)t 커널 크기 3x3보다 훨씬 큽니다. 여기서 우리는 ConvNet을 위한 큰 커널 크기 컨볼루션의 사용을 재검토합니다.

#### 깊이별 합성곱 레이어 위로 이동: 
큰 커널을 탐색하기 위한 전제 조건 중 하나는 깊이별 컨볼루션 레이어의 위치를 위로 이동하는 것입니다(그림 3 (b)에서 (c)). 이는 Transformer에서도 분명한 설계 결정입니다. MSA 블록은 MLP 레이어 이전에 배치됩니다. inverted bottleneck 블록이 있으므로 복잡/비효율적인 모듈(MSA, large-kernel conv)은 더 적은 채널을 갖는 반면 효율적이고 조밀한 1x1 레이어는 무거운 작업을 수행하기 때문에 이는 자연스러운 설계 선택입니다. 이 중간 단계는 FLOP를 4.1G로 줄여 일시적으로 성능이 79.9%로 저하됩니다. \
![figure3]()

#### 커널 크기 늘리기: 
이러한 모든 준비를 통해 더 큰 커널 크기의 컨볼루션을 채택하면 상당한 이점이 있습니다. 3, 5, 7, 9 및 11을 포함하여 여러 커널 크기를 실험했습니다. 네트워크의 성능은 79.9%(3x3)에서 80.6%(7x7)로 증가하는 반면 네트워크의 FLOP는 거의 동일하게 유지됩니다. 또한 더 큰 커널 크기의 이점은 7x7에서 포화점에 도달한다는 것을 관찰했습니다. 대용량 모델에서도 이러한 동작을 확인했습니다. ResNet-200 체제 모델은 커널 크기를 7x7 이상으로 늘려도 더 이상 이득을 얻지 못합니다.

각 블록에서 7x7 깊이별 컨볼루션을 사용합니다.

이 시점에서 매크로 규모의 네트워크 아키텍처 검토를 마쳤습니다. 흥미롭게도 비전 트랜스포머에서 채택된 설계 선택의 상당 부분은 ConvNet 인스턴스화에 매핑될 수 있습니다.


## 2.6. Micro Design
이 섹션에서는 마이크로 수준에서 몇 가지 다른 아키텍처 차이점을 조사합니다. 여기에서 대부분의 탐색은 활성화 함수 및 정규화 계층의 특정 선택에 초점을 맞춰 레이어 수준에서 수행됩니다.

### ReLU를 GELU로 교체
: NLP 아키텍처와 비전 아키텍처 간의 한 가지 불일치는 사용할 활성화 함수의 세부 사항입니다. 시간이 지남에 따라 수많은 활성화 함수가 개발되었지만 Rectified Linear Unit(ReLU) [49]은 단순성과 효율성으로 인해 ConvNet에서 여전히 광범위하게 사용됩니다. ReLU는 원래 Transformer 논문 [77]에서 활성화 함수로도 사용됩니다. ReLU의 더 부드러운 변형으로 생각할 수 있는 GELU(Gaussian Error Linear Unit) [32]는 Google BERT [18] 및 OpenAI의 GPT-2 [52]를 포함하여 가장 진보된 Transformer에서 사용됩니다. 그리고 가장 최근에는 ViT에도 사용됩니다. ConvNet에서도 ReLU를 GELU로 대체할 수 있지만 정확도는 변경되지 않습니다(80.6%).

## 더 적은 활성화 함수: 
Transformer와 ResNet 블록 간의 사소한 차이점 중 하나는 Transformer에 활성화 함수가 더 적다는 것입니다. 키/쿼리/값 선형 임베딩 레이어, 투영 레이어 및 MLP 블록의 두 개의 선형 레이어가 있는 Transformer 블록을 고려하십시오. MLP 블록에는 활성화 함수가 하나만 있습니다. 이에 비해 1x1 conv를 포함하여 각 컨볼루션 레이어에 활성화 함수를 추가하는 것이 일반적입니다. 여기서 우리는 동일한 전략을 고수할 때 성능이 어떻게 변하는지 조사합니다. 그림 4에 나와 있는 것처럼 Transformer 블록 스타일을 복제하여 두 개의 1x1 레이어 사이에 하나를 제외한 residual 블록에서 모든 GELU 레이어를 제거합니다. 이 과정은 결과를 0.7%에서 81.3%로 향상시켜 Swin-T의 성능과 거의 일치시킵니다. 이제 각 블록에 단일 GELU 활성화를 사용합니다.

### 더 적은 정규화 레이어: 
Transformer 블록에는 일반적으로 정규화 레이어도 더 적습니다. 여기에서 conv 1x1 레이어 앞에 하나의 BN 레이어만 남겨두고 두 개의 BatchNorm(BN) 레이어를 제거합니다. 이렇게 하면 이미 Swin-T의 결과를 능가하는 81.4%로 성능이 더욱 향상됩니다. 블록 시작 부분에 하나의 추가 BN 레이어를 추가해도 성능이 향상되지 않는다는 것을 경험적으로 알 수 있으므로 Transformer보다 블록당 정규화 레이어가 훨씬 더 적습니다.

#### BN을 LN으로 대체: 
배치 정규화(BN) [38]은 수렴을 개선하고 과적합을 줄이기 때문에 ConvNet의 필수 구성 요소입니다. 그러나 BN에는 모델 성능에 악영향을 줄 수 있는 많은 복잡성도 있습니다 [84]. 대체 정규화 [60, 75, 83] 기술을 개발하려는 수많은 시도가 있었지만 BN은 대부분의 비전 작업에서 선호되는 옵션으로 남아 있습니다. 반면에 더 간단한 Layer Normalization [5] (LN)은 Transformer에서 사용되어 다양한 응용 시나리오에서 좋은 성능을 얻었습니다. 원래 ResNet에서 LN을 BN으로 직접 대체하면 성능이 저하됩니다 [83]. 네트워크 아키텍처 및 교육 기술의 모든 수정 사항과 함께 여기에서 BN 대신 LN을 사용하는 영향을 다시 살펴봅니다. ConvNet 모델은 LN으로 훈련하는 데 어려움이 없습니다. 사실 성능이 약간 더 좋으며 81.5%의 정확도를 얻습니다. 지금부터 각 residual 블록에서 정규화 선택으로 하나의 LayerNorm을 사용합니다.

#### 별도의 다운샘플링 레이어: 
ResNet에서 공간 다운샘플링은 각 단계 시작 부분에 있는 residual 블록에서 보폭 2의 3x3 conv(및 shortcut 연결에서 보폭 2의 1x1 conv)를 사용하여 수행됩니다. Swin Transformers에서는 별도의 다운샘플링 레이어가 스테이지 사이에 추가됩니다. 공간 다운샘플링을 위해 보폭 2의 2x2 conv 레이어를 사용하는 유사한 전략을 탐색합니다. 이 수정은 놀랍게도 훈련이 분기됩니다. 추가 조사에 따르면 공간 해상도가 변경될 때마다 정규화 레이어를 추가하면 교육 안정화에 도움이 될 수 있습니다. 여기에는 Swin Transformers에서도 사용되는 여러 LN 레이어가 포함됩니다. 각 다운샘플링 레이어 앞에 하나, 줄기 뒤에 하나, 마지막 글로벌 평균 풀링 후에 하나입니다. 정확도를 82.0%로 높여 Swin-T의 81.3%를 크게 웃돌 수 있습니다. 별도의 다운샘플링 레이어를 사용합니다. 이것이 ConvNeXt라고 불리는 최종 모델입니다.

ResNet, Swin 및 ConvNeXt 블록 구조 비교는 그림 4에서 찾을 수 있습니다. ResNet-50, Swin-T 및 ConvNeXt-T의 자세한 아키텍처 사양 비교는 표 9에서 찾을 수 있습니다.

#### 마무리 발언: 
첫 번째 "플레이 스루"를 마치고 순수 ConvNet인 ConvNeXt를 발견했습니다. 이 컴퓨팅 체제에서 ImageNet-1K 분류를 위해 Swin Transformer보다 성능이 뛰어날 수 있습니다. 지금까지 논의된 모든 설계 선택은 비전 트랜스포머에서 채택되었습니다. 또한 이러한 디자인은 ConvNet 문헌에서도 새로운 것이 아닙니다. 지난 10년 동안 모두 별도로 연구되었지만 집합적으로 연구되지는 않았습니다. ConvNeXt 모델은 Swin Transformer와 거의 동일한 FLOP, #params., 처리량 및 메모리 사용량을 갖지만 이동된 윈도우 주의 또는 상대 위치 편향과 같은 특수 모듈이 필요하지 않습니다.

이러한 발견은 고무적이지만 아직 완전히 설득력이 있는 것은 아닙니다. 지금까지 우리의 탐구는 소규모로 제한되었지만 비전 트랜스포머의 확장 행동은 진정으로 그들을 구별하는 것입니다. 또한 ConvNet이 객체 감지 및 의미 분할과 같은 다운스트림 작업에서 Swin Transformers와 경쟁할 수 있는지 여부는 컴퓨터 비전 실무자들의 주요 관심사입니다. 다음 섹션에서는 ConvNeXt 모델의 데이터 및 모델 크기를 확장하고 다양한 시각적 인식 작업 세트에서 평가합니다.





















---